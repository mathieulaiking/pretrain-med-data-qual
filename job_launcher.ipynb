{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e7beae-3b06-4282-93e3-b9c7ac918ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from idr_pytools import gpu_jobs_submitter\n",
    "\n",
    "# project root path\n",
    "dsdir = os.getenv(\"DSDIR\")\n",
    "scratch = os.getenv(\"SCRATCH\")\n",
    "\n",
    "root = os.path.join(scratch,\"pretrain-med-data-qual\")\n",
    "idr_models_dir = os.path.join(dsdir,\"HuggingFace_Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaaa70-65b5-4f87-8983-4f39f5f2a0bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# bert mlm pretraining continual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bec35-8863-41ef-a311-343f4f1b1d7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## defining arguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893beaa8-3b21-4ea4-8004-7f1cc3794559",
   "metadata": {},
   "source": [
    "The Pretraining Phases Hardware arguments are taken from [NVIDIA Pytorch BERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#pre-training-nvidia-dgx-a100-8x-a100-80gb) and [NVIDIA Tensorflow BioBERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/biobert/README.md#pre-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e357672-8722-46ea-bf07-2a404a8aab0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "phase = 1\n",
    "\n",
    "# Local Paths\n",
    "pubmed_path = f\"{root}/data/pubmed_preproc\"\n",
    "bert_path = f\"{idr_models_dir}/bert-base-uncased\"\n",
    "run_mlm_path = f\"{root}/pretraining/run_mlm_offline.py \"\n",
    "accuracy_path = f\"{root}/pretraining/accuracy.py\"\n",
    "\n",
    "out_dir_template = f\"{root}/pretraining/{{exp_name}} \"\n",
    "\n",
    "# Pretraining Phases Hardware Arguments \n",
    "if debug :\n",
    "    gpu = \"a100\"\n",
    "    n_gpu = 4\n",
    "    sequence_length = 128\n",
    "    batch_size = 128\n",
    "    precision = \"fp16\"\n",
    "    max_steps = 100\n",
    "    model_path = bert_path\n",
    "    acc_steps = None # gradient accumulation steps\n",
    "    \n",
    "elif phase == 1 :\n",
    "    gpu = \"a100\"\n",
    "    n_gpu = 8\n",
    "    sequence_length = 128\n",
    "    precision = \"fp16\" # tf32,bf16\n",
    "    batch_size = 256 # 128 if tf32\n",
    "    max_steps = 19531\n",
    "    acc_steps = None # 32 according to NVIDIA\n",
    "    model_path = bert_path\n",
    "    \n",
    "elif phase == 2 :\n",
    "    gpu = \"a100\"\n",
    "    n_gpu = 8\n",
    "    sequence_length = 512\n",
    "    batch_size = 32\n",
    "    precision = \"fp16\" # tf32,bf16\n",
    "    max_steps = 4340\n",
    "    acc_steps = None # 128 according to NVIDIA\n",
    "    model_path = \"{model_path}\" # must be last checkpoint of phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4ae9a2-64dd-485c-82c3-6a7303cfc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = \"torchrun --standalone \"\n",
    "base_cmd += f\"--nproc_per_node {n_gpu} \"\n",
    "base_cmd += \"--nnodes 1 \"\n",
    "base_cmd += f\"{run_mlm_path} \"\n",
    "\n",
    "# Model Arguments\n",
    "base_cmd += f\"--model_name_or_path {model_path} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += f\"--dataset_name {pubmed_path} \"\n",
    "base_cmd += f\"--metric_path {accuracy_path} \"\n",
    "base_cmd += f\"--max_eval_samples {25600} \" if debug else \"\"\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {sequence_length} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train \" \n",
    "base_cmd += \"--do_eval \"  \n",
    "base_cmd += \"--seed 42 \" \n",
    "base_cmd += \"--overwrite_output_dir true \" # if debug else \"\" \n",
    "## BERT hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 1e-4 \" \n",
    "base_cmd += \"--weight_decay 0.01 \" \n",
    "base_cmd += \"--adam_beta1 0.9 \" \n",
    "base_cmd += \"--adam_beta2 0.999 \" \n",
    "base_cmd += \"--adam_epsilon 1e-6 \" # RoBERTa\n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 1 \"\n",
    "base_cmd += f\"--gradient_accumulation_steps {acc_steps} \" if acc_steps else \"\"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--max_steps {max_steps} \"\n",
    "base_cmd += f\"--warmup_steps {max_steps//10} \" # warmup for 10% of steps\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy no \" # no evaluation during training only at the end for perplexity\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--log_on_each_node false \"\n",
    "base_cmd += \"--save_total_limit 1 \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Data Filters experiences\n",
    "bounds = {\n",
    "    \"none\":[\n",
    "        (None,None,\"all\"),\n",
    "    ],\n",
    "    \"random\":[\n",
    "        (0.0,0.5,\"50%\"),\n",
    "        (0.0,0.25,\"25%\")\n",
    "    ],\n",
    "    \"h-index\":[\n",
    "        (103,1400,\"top50%\"),\n",
    "        (53,190,\"mid50%\"),\n",
    "        (190,1400,\"top25%\"),\n",
    "        (77,142,\"mid25%\"),\n",
    "    ],\n",
    "    \"sjr\":[\n",
    "        (1.312,100.0,\"top25%\"),\n",
    "        (0.462,0.984,\"mid25%\"),\n",
    "    ]\n",
    "}\n",
    "cmds = []\n",
    "exp_names = []\n",
    "for metric, exps in bounds.items():\n",
    "    for lower_bound, upper_bound, bound_name in exps:\n",
    "        cmd = base_cmd\n",
    "        # filtering metric\n",
    "        if metric != \"none\": \n",
    "            cmd += f\"--filter_metric {metric} \"\n",
    "            cmd += f\"--filter_lower_threshold {lower_bound} \"\n",
    "            cmd += f\"--filter_upper_threshold {upper_bound} \"\n",
    "        # experience name\n",
    "        exp_name = f\"{metric}_{bound_name}_p{phase}\" if not debug else f\"{metric}_{bound_name}_debug\"\n",
    "        # output_dir\n",
    "        cmd += f\"--output_dir  {out_dir_template.format(exp_name=exp_name)}\"\n",
    "        # wandb args\n",
    "        cmd += f\"--wandb_group {exp_name}_{gpu}x{n_gpu} \"\n",
    "        cmd += f\"--wandb_name {exp_name} \"\n",
    "        # model resuming for phase 2\n",
    "        if phase == 2: # TODO : find last checkpoint for phase 2\n",
    "            pass\n",
    "        # append to lists\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "        \n",
    "\n",
    "cmds = [cmds[-1]]\n",
    "exp_names = [exp_names[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d07e-4f5e-47ce-91be-b917ce65a387",
   "metadata": {
    "tags": []
   },
   "source": [
    "## launching jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c382f9-5cb3-431c-b57b-a95ac5e5b2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 4 GPUs distributed on 1 nodes with 4 tasks / 4 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1551459\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = f\"\"\"module load python/3.11.5\n",
    "conda activate transformers_latest\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :\n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\" if \"v100\" in gpu else gpu,\n",
    "        time_max=\"20:00:00\" if not debug else \"2:00:00\",\n",
    "        account=f\"aro@{gpu}\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb31a57f-d669-422a-8d0a-768593c43fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1551459    gpu_p5 sjr_mid2  urz45id PD       0:00      1 (Resources)\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22283c3-4a80-4786-9417-24d21936fae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/debug-cli.urz45id.log\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/1fqffbgi ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/fq24fblr ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/10h8k03p ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/27y4t73n ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/6kmn3f36 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/d7ywq1jb ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/jlqwfc73 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/lzh5imzj ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/srtbt852 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/sx019rtj ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# sync weigths and biases\n",
    "# TODO : handle distributed logging (one wandb run for each GPU used currently)\n",
    "!wandb sync --include-offline wandb/offline-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd455b5-6404-4ef1-bde8-581b69616f1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# blurb ner evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69123ad3-bd4b-4e9f-8a05-76e622224901",
   "metadata": {
    "tags": []
   },
   "source": [
    "## define arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbce7e-4a37-4f76-818d-37456feb9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "gpu = \"v100-32g\"\n",
    "\n",
    "base_cmd = f\"python {root}/evaluation/run_ner.py \"\n",
    "\n",
    "# Model Arguments\n",
    "base_cmd += f\"--model_name_or_path {scratch}/models/bert-base-uncased \"\n",
    "base_cmd += f\"--cache_dir {scratch}/hf_cache \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--dataset_name bigbio/blurb \"\n",
    "base_cmd += f\"--seqeval_metric_path {root}/evaluation/seqeval.py \"\n",
    "base_cmd += \"--max_seq_length 512 \"\n",
    "base_cmd += \"--pad_to_max_length true \" if debug else \"\"\n",
    "base_cmd += \"--max_train_samples 64 \" if debug else \"\"\n",
    "base_cmd += \"--max_eval_samples 64 \" if debug else \"\"\n",
    "base_cmd += \"--preprocessing_num_workers 10 \" if debug else \"\"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments\n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--seed 42 \"\n",
    "## Efficiency\n",
    "base_cmd += \"--tf32 true \" if gpu == \"a100\" else \"\"\n",
    "base_cmd += \"--bf16 false \"\n",
    "base_cmd += \"--fp16 true \" if gpu == \"v100\" else \"\"\n",
    "## Batchs\n",
    "base_cmd += \"--per_device_train_batch_size 32 \"\n",
    "base_cmd += \"--per_device_eval_batch_size 32 \"\n",
    "## Optimizer (default : linear with warmup)\n",
    "base_cmd += \"--learning_rate 2e-5 \" # SciBERT  \n",
    "base_cmd += \"--weight_decay 0.01 \"  \n",
    "base_cmd += \"--adam_beta1 0.9 \" \n",
    "base_cmd += \"--adam_beta2 0.98 \" \n",
    "base_cmd += \"--warmup_ratio 0.1 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += \"--num_train_epochs 10 \" \n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--evaluation_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--logging_steps 0.1 \" if not debug else \"--logging_steps 0.5 \"\n",
    "base_cmd += \"--eval_steps 0.1 \" if not debug else \"--eval_steps 0.5 \"\n",
    "base_cmd += \"--save_steps 0.1 \" if not debug else \"--save_steps 0.5 \"\n",
    "base_cmd += \"--log_on_each_node false \"\n",
    "base_cmd += \"--local_rank 0 \"\n",
    "base_cmd += \"--save_total_limit 3 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to none \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6780b1-6485-4c46-ae00-723eac99ccb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_seeds = 5 # 10 for BioASQ, BIOSSES, and PubMedQA in PubMedBERT paper and 5 for others\n",
    "blurb_ner_configs = [\"bc5chem\",\"bc5disease\",\"bc2gm\",\"jnlpba\",\"ncbi_disease\"]\n",
    "\n",
    "cmds = []\n",
    "for blurb_config in blurb_ner_configs:\n",
    "    cmd = base_cmd\n",
    "    cmd += f\"--dataset_config_name {blurb_config} \"\n",
    "    for seed in range(n_seeds):\n",
    "        cmd += \"--seed 42 \"\n",
    "        if debug:\n",
    "            cmd += \"--output_dir evaluation/debug_output \"\n",
    "        else:\n",
    "            cmd += f\"--output_dir evaluation/bert-base-uncased/{blurb_config}/{seed} \"\n",
    "        cmds.append(cmd)\n",
    "\n",
    "if debug : \n",
    "    cmds = cmds[0]\n",
    "\n",
    "cmds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22051d-4f0d-4832-9a18-f6eb6aab90e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## launch jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb357f-977b-4ae3-a956-0f5e67d7210f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = \"blurb-ner-debug\" if debug else \"blurb-ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ebe27-0268-4e1b-bead-6a6a5f53f0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_ids = gpu_jobs_submitter(\n",
    "    cmds , # ONE JOB TO TEST ONLY\n",
    "    name=job_name,\n",
    "    module=\"pytorch-gpu/py3/2.1.1\",\n",
    "    n_gpu = 1,\n",
    "    qos=\"qos_gpu-dev\",\n",
    "    time_max=\"02:00:00\" if debug else \"5:00:00\",\n",
    "    account=f\"aro@{gpu.split('-')[0]}\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f640be-90f6-46b7-a8a3-ee3b078f5f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# clean logs, cache , wandb runs, cancel jobs, training debug checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6f387b2-27ac-4919-a0b3-66693ff420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete logs\n",
    "!rm -rf slurm/log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e795ecc8-a51d-402f-8366-876311b3b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete debug training checkpoints\n",
    "!rm -rf pretraining/*_debug/checkpoint*\n",
    "!rm -rf pretraining/*_debug/*.bin\n",
    "!rm -rf pretraining/*_debug/*.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51326a-116b-452b-973e-a73eb55a3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete wandb run dir\n",
    "!rm -rf wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36513978-d88d-4dc5-96ab-087635cc1776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove data cache and tmp files\n",
    "!rm -rf data/.cache\n",
    "!rm -rf data/pubmed_preproc/*/cache-*.arrow\n",
    "!rm -rf data/pubmed_preproc/*/tmp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b414574-1b8e-4c1c-93d3-ffef13acfb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf core-python-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3e0c2-b215-4a7b-8997-0ab5703d8749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cancel all my jobs\n",
    "!scancel -u $USER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.2.0_py3.11.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.2.0_py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
