{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e7beae-3b06-4282-93e3-b9c7ac918ac6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "from idr_pytools import gpu_jobs_submitter\n",
    "\n",
    "# project root path\n",
    "dsdir = os.getenv(\"DSDIR\")\n",
    "scratch = os.getenv(\"SCRATCH\")\n",
    "root = os.path.join(scratch,\"pretrain-med-data-qual\")\n",
    "idr_models_dir = os.path.join(dsdir,\"HuggingFace_Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaaa70-65b5-4f87-8983-4f39f5f2a0bd",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# bert mlm pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8f86e-be94-4339-8be5-d7dc5ffd58fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## computing number of optimal steps and grad accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171c5ae-b85e-4a3e-8db0-9d2a1d2a1300",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define the number of optimal steps as the number of steps required to perform an entire epoch on the full PubMed dataset (Baseline last update january 2024).\n",
    "Following RoBERTa, we aim for an effective batch_size of 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efafc7-412f-4796-a8b4-bb2fee4e2710",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Pretraining Phases Hardware arguments are taken from [NVIDIA Pytorch BERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#pre-training-nvidia-dgx-a100-8x-a100-80gb) and [NVIDIA Tensorflow BioBERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/biobert/README.md#pre-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e078cec7-86cb-4b17-8beb-6ee86920490b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation needed for effective batch size of 8192, with 2*a100 GPUs with per device batch size of 32 =  128\n",
      "Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : 4194304\n",
      "Optimal step number with effective batch size of 8192 : 3598.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient accumulation\n",
    "sequence_length = 512\n",
    "gpu_model = \"a100\"\n",
    "max_batch_size_per_gpu = 32 # on A100 with 512 seq length\n",
    "gpu_nb = 2\n",
    "target_batch_size = 8192\n",
    "gradient_accumulation = target_batch_size // (gpu_nb*max_batch_size_per_gpu) \n",
    "print(f\"Gradient accumulation needed for effective batch size of {target_batch_size}, with {gpu_nb}*{gpu_model} GPUs with per device batch size of {max_batch_size_per_gpu} = \",gradient_accumulation)\n",
    "# Optimal steps number\n",
    "total_token_nb =  15888466068 # calculated number of tokens in pubmed\n",
    "train_token_nb = 0.95*total_token_nb\n",
    "token_per_step = target_batch_size * sequence_length\n",
    "optimal_train_step_nb = train_token_nb // token_per_step\n",
    "print(f\"Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : {token_per_step}\")\n",
    "print(f\"Optimal step number with effective batch size of {target_batch_size} : {optimal_train_step_nb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bec35-8863-41ef-a311-343f4f1b1d7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## defining arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e357672-8722-46ea-bf07-2a404a8aab0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "pubmed_path = f\"{root}/data/pubmed_preproc\"\n",
    "bert_path = f\"{idr_models_dir}/bert-base-uncased\"\n",
    "run_mlm_path = f\"{root}/pretraining/run_mlm_offline.py \"\n",
    "accuracy_path = f\"{root}/pretraining/accuracy.py\"\n",
    "out_dir_template = f\"{root}/pretraining/{{exp_name}}\"\n",
    "\n",
    "# Pretraining Phases Hardware Arguments for A100 GPUs\n",
    "n_gpu = 2\n",
    "sequence_length = 512\n",
    "batch_size = 32 # per device\n",
    "precision = \"fp16\"\n",
    "max_steps = 3598 if not debug else 100\n",
    "acc_steps = 128 if not debug else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4ae9a2-64dd-485c-82c3-6a7303cfc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none_all run train and eval\n",
      "random_50% already finished\n",
      "random_25% already finished\n",
      "h-index_top50% already finished\n",
      "h-index_mid50% already finished\n",
      "h-index_top25% already finished\n",
      "h-index_mid25% already finished\n",
      "sjr_top25% already finished\n",
      "sjr_mid25% already finished\n",
      "sjr_top50% run eval only\n",
      "0 : none_all\n",
      "1 : sjr_top50%\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = \"torchrun --standalone \"\n",
    "base_cmd += f\"--nproc_per_node {n_gpu} \"\n",
    "base_cmd += \"--nnodes 1 \"\n",
    "base_cmd += f\"{run_mlm_path} \"\n",
    "\n",
    "# Model Arguments\n",
    "base_cmd += f\"--model_name_or_path {bert_path} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += f\"--dataset_name {pubmed_path} \"\n",
    "base_cmd += f\"--metric_path {accuracy_path} \"\n",
    "base_cmd += f\"--max_eval_samples {25600} \" if debug else \"\"\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {sequence_length} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--seed 42 \" \n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## BERT hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 1e-4 \" \n",
    "base_cmd += \"--weight_decay 0.01 \" \n",
    "base_cmd += \"--adam_beta1 0.9 \" \n",
    "base_cmd += \"--adam_beta2 0.999 \" \n",
    "base_cmd += \"--adam_epsilon 1e-6 \" # RoBERTa\n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "base_cmd += f\"--gradient_accumulation_steps {acc_steps} \" if acc_steps else \"\"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--max_steps {max_steps} \"\n",
    "base_cmd += f\"--warmup_steps {max_steps//10} \" # warmup for 10% of steps\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy no \" # no evaluation during training only at the end for perplexity\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--logging_steps 0.01 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--log_on_each_node false \"\n",
    "base_cmd += \"--log_on_each_node false \"\n",
    "base_cmd += \"--save_total_limit 3 \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Data Filters experiences\n",
    "bounds = {\n",
    "    \"none\":[\n",
    "        (None,None,\"all\"),\n",
    "    ],\n",
    "    \"random\":[\n",
    "        (0.0,0.5,\"50%\"),\n",
    "        (0.0,0.25,\"25%\")\n",
    "    ],\n",
    "    \"h-index\":[\n",
    "        (103,1400,\"top50%\"),\n",
    "        (53,190,\"mid50%\"),\n",
    "        (190,1400,\"top25%\"),\n",
    "        (77,142,\"mid25%\"),\n",
    "    ],\n",
    "    \"sjr\":[\n",
    "        (1.312,100.0,\"top25%\"),\n",
    "        (0.462,0.984,\"mid25%\"),\n",
    "        (0.759,100.0,\"top50%\"),\n",
    "    ]\n",
    "}\n",
    "cmds = []\n",
    "exp_names = []\n",
    "for metric, exps in bounds.items():\n",
    "    for lower_bound, upper_bound, bound_name in exps:\n",
    "        cmd = base_cmd\n",
    "        # filtering metric\n",
    "        if metric != \"none\": \n",
    "            cmd += f\"--filter_metric {metric} \"\n",
    "            cmd += f\"--filter_lower_threshold {lower_bound} \"\n",
    "            cmd += f\"--filter_upper_threshold {upper_bound} \"\n",
    "        else:\n",
    "            cmd += \"--streaming \"\n",
    "        # experience name\n",
    "        exp_name = f\"{metric}_{bound_name}\" \n",
    "        if debug : exp_name += \"_debug\"\n",
    "        # evaluate cache dir\n",
    "        cmd += f\"--evaluate_cache_dir pretraining/.evaluate_cache/{exp_name} \"\n",
    "        # output_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        cmd += f\"--output_dir {out_dir} \"\n",
    "        # wandb args\n",
    "        cmd += f\"--wandb_group {exp_name} \"\n",
    "        cmd += f\"--wandb_name {exp_name} \"\n",
    "        # append to lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"eval_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        elif os.path.exists(os.path.join(out_dir,\"train_results.json\")):\n",
    "            print(exp_name,\"run eval only\")\n",
    "            cmd += \"--do_eval \"  \n",
    "        else : \n",
    "            print(exp_name,\"run train and eval\")\n",
    "            cmd += \"--do_train \" \n",
    "            cmd += \"--do_eval \" \n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "        \n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc582b13-fa85-4d71-a10f-8b662dcde050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if debug : \n",
    "    debug_ind = 4\n",
    "    cmds = [cmds[debug_ind]]\n",
    "    exp_names = [exp_names[debug_ind]]\n",
    "    print(cmds)\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d07e-4f5e-47ce-91be-b917ce65a387",
   "metadata": {
    "tags": []
   },
   "source": [
    "## launching jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c382f9-5cb3-431c-b57b-a95ac5e5b2cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1977363\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1977364\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = \"\"\"module load python/3.11.5\n",
    "conda activate pretrain\n",
    "\"\"\"\n",
    "for cmd,exp_name in zip(cmds, exp_names) :\n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"a100\",\n",
    "        time_max=\"20:00:00\" if not debug else \"2:00:00\",\n",
    "        account=f\"aro@a100\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd455b5-6404-4ef1-bde8-581b69616f1d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# fine-tune blurb eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb07337-e312-45e2-bdb6-a1eb7d200ad8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = f\"{root}/data/.blurb_cache\"\n",
    "out_dir_template = f\"{root}/evaluation/out/{{exp_name}}\"\n",
    "models_paths = [\n",
    "    f\"{idr_models_dir}/bert-base-uncased\",\n",
    "    f\"{root}/pretraining/none_all_ckpt-1440\",\n",
    "    f\"{root}/pretraining/random_25%\",\n",
    "    f\"{root}/pretraining/random_50%_ckpt-3240\",\n",
    "    f\"{root}/pretraining/h-index_mid25%\",\n",
    "    f\"{root}/pretraining/h-index_mid50%\",\n",
    "    f\"{root}/pretraining/h-index_top25%\",\n",
    "    f\"{root}/pretraining/h-index_top50%_ckpt-3240\",\n",
    "    f\"{root}/pretraining/sjr_mid25%\",\n",
    "    f\"{root}/pretraining/sjr_top25%\",\n",
    "    f\"{root}/pretraining/sjr_top50%_ckpt-2880\",\n",
    "]\n",
    "\n",
    "# Pretraining Phases Hardware Arguments \n",
    "# for 1xV100\n",
    "max_seq_length = 512\n",
    "batch_size = 16 # per device\n",
    "precision = \"fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69123ad3-bd4b-4e9f-8a05-76e622224901",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "808de476-3a97-4ad5-8c5c-8d93eea067ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "run_ner_path = f\"{root}/evaluation/run_ner.py \"\n",
    "seqeval_path = f\"{root}/evaluation/metrics/evaluate_seqeval.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7cbce7e-4a37-4f76-818d-37456feb9493",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_ner_path} \"\n",
    "\n",
    "# Model args\n",
    "base_cmd += f\"--cache_dir {cache_dir} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--seqeval_path {seqeval_path} \"\n",
    "base_cmd += \"--return_entity_level_metrics \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Different experiments Runs \n",
    "datasets_configs=[\n",
    "    (\"bigbio/blurb\",\"bc5chem\"),\n",
    "    (\"bigbio/blurb\",\"bc5disease\"),\n",
    "    (\"bigbio/blurb\",\"bc2gm\"),\n",
    "    (\"bigbio/blurb\",\"jnlpba\"),\n",
    "    (\"bigbio/blurb\",\"ncbi_disease\"),\n",
    "]\n",
    "seed_nb = 5\n",
    "\n",
    "cmds = []\n",
    "exp_names = []\n",
    "\n",
    "for model_path in models_paths :\n",
    "    for dataset_name, dataset_config in datasets_configs :\n",
    "        for seed in range(seed_nb):\n",
    "            cmd = base_cmd\n",
    "            cmd += f\"--dataset_name {dataset_name} \"\n",
    "            cmd += f\"--dataset_config_name {dataset_config} \" if dataset_config else \"\"\n",
    "            cmd += f\"--seed {seed} \"\n",
    "            cmd += f\"--model_name_or_path {model_path} \"\n",
    "            \n",
    "            # Experience name for output directory\n",
    "            exp_name = f\"{model_path.split('/')[-1]}_{dataset_name.split('/')[-1]}\"\n",
    "            exp_name += f\"-{dataset_config}\" if dataset_config else \"\"\n",
    "            exp_name += f\"_seed{seed}\"\n",
    "            exp_name += \"_debug\" if debug else \"\"\n",
    "            out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "            cmd += f\"--output_dir {out_dir} \"\n",
    "            # Cache dir (for cache problems)\n",
    "            base_cmd += f\"--evaluate_cache_dir {root}/evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "            # Weights and Biases\n",
    "            cmd += f\"--run_name {exp_name}\"\n",
    "            # fill lists\n",
    "            if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "                #print(exp_name, \"already finished\")\n",
    "                continue\n",
    "            cmds.append(cmd)\n",
    "            exp_names.append(exp_name)\n",
    "\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 24 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2d0d3d-424b-4694-9710-b81949dd93e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941844\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941845\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941848\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941849\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941852\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941853\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941855\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941856\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941858\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941861\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941864\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941865\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941867\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941870\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941872\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941873\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941875\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941876\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941878\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941880\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941882\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941883\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941884\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941886\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941887\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = f\"\"\"module load python/3.11.5\n",
    "conda activate pretrain\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cc9b0-3a23-4d02-a369-9dcde2f7662d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## biosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2736f356-2270-4810-a98c-ecdf862267e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "run_biosses_path = f\"{root}/evaluation/run_biosses.py\"\n",
    "biosses_path = f\"{root}/data/biosses\"\n",
    "pearsonr_path = f\"{root}/evaluation/metrics/evaluate_pearsonr.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63c27598-9343-45f4-9a48-ecec9ae41379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_biosses_seed0_debug already finished\n",
      "random_25%_biosses_seed0_debug already finished\n",
      "0 : bert-base-uncased_biosses_seed1_debug\n",
      "1 : bert-base-uncased_biosses_seed2_debug\n",
      "2 : bert-base-uncased_biosses_seed3_debug\n",
      "3 : bert-base-uncased_biosses_seed4_debug\n",
      "4 : bert-base-uncased_biosses_seed5_debug\n",
      "5 : bert-base-uncased_biosses_seed6_debug\n",
      "6 : bert-base-uncased_biosses_seed7_debug\n",
      "7 : bert-base-uncased_biosses_seed8_debug\n",
      "8 : bert-base-uncased_biosses_seed9_debug\n",
      "9 : random_25%_biosses_seed1_debug\n",
      "10 : random_25%_biosses_seed2_debug\n",
      "11 : random_25%_biosses_seed3_debug\n",
      "12 : random_25%_biosses_seed4_debug\n",
      "13 : random_25%_biosses_seed5_debug\n",
      "14 : random_25%_biosses_seed6_debug\n",
      "15 : random_25%_biosses_seed7_debug\n",
      "16 : random_25%_biosses_seed8_debug\n",
      "17 : random_25%_biosses_seed9_debug\n",
      "18 : h-index_mid25%_biosses_seed0_debug\n",
      "19 : h-index_mid25%_biosses_seed1_debug\n",
      "20 : h-index_mid25%_biosses_seed2_debug\n",
      "21 : h-index_mid25%_biosses_seed3_debug\n",
      "22 : h-index_mid25%_biosses_seed4_debug\n",
      "23 : h-index_mid25%_biosses_seed5_debug\n",
      "24 : h-index_mid25%_biosses_seed6_debug\n",
      "25 : h-index_mid25%_biosses_seed7_debug\n",
      "26 : h-index_mid25%_biosses_seed8_debug\n",
      "27 : h-index_mid25%_biosses_seed9_debug\n",
      "28 : sjr_top25%_biosses_seed0_debug\n",
      "29 : sjr_top25%_biosses_seed1_debug\n",
      "30 : sjr_top25%_biosses_seed2_debug\n",
      "31 : sjr_top25%_biosses_seed3_debug\n",
      "32 : sjr_top25%_biosses_seed4_debug\n",
      "33 : sjr_top25%_biosses_seed5_debug\n",
      "34 : sjr_top25%_biosses_seed6_debug\n",
      "35 : sjr_top25%_biosses_seed7_debug\n",
      "36 : sjr_top25%_biosses_seed8_debug\n",
      "37 : sjr_top25%_biosses_seed9_debug\n",
      "38 : sjr_mid25%_biosses_seed0_debug\n",
      "39 : sjr_mid25%_biosses_seed1_debug\n",
      "40 : sjr_mid25%_biosses_seed2_debug\n",
      "41 : sjr_mid25%_biosses_seed3_debug\n",
      "42 : sjr_mid25%_biosses_seed4_debug\n",
      "43 : sjr_mid25%_biosses_seed5_debug\n",
      "44 : sjr_mid25%_biosses_seed6_debug\n",
      "45 : sjr_mid25%_biosses_seed7_debug\n",
      "46 : sjr_mid25%_biosses_seed8_debug\n",
      "47 : sjr_mid25%_biosses_seed9_debug\n",
      "48 : h-index_mid50%_biosses_seed0_debug\n",
      "49 : h-index_mid50%_biosses_seed1_debug\n",
      "50 : h-index_mid50%_biosses_seed2_debug\n",
      "51 : h-index_mid50%_biosses_seed3_debug\n",
      "52 : h-index_mid50%_biosses_seed4_debug\n",
      "53 : h-index_mid50%_biosses_seed5_debug\n",
      "54 : h-index_mid50%_biosses_seed6_debug\n",
      "55 : h-index_mid50%_biosses_seed7_debug\n",
      "56 : h-index_mid50%_biosses_seed8_debug\n",
      "57 : h-index_mid50%_biosses_seed9_debug\n",
      "58 : h-index_top25%_biosses_seed0_debug\n",
      "59 : h-index_top25%_biosses_seed1_debug\n",
      "60 : h-index_top25%_biosses_seed2_debug\n",
      "61 : h-index_top25%_biosses_seed3_debug\n",
      "62 : h-index_top25%_biosses_seed4_debug\n",
      "63 : h-index_top25%_biosses_seed5_debug\n",
      "64 : h-index_top25%_biosses_seed6_debug\n",
      "65 : h-index_top25%_biosses_seed7_debug\n",
      "66 : h-index_top25%_biosses_seed8_debug\n",
      "67 : h-index_top25%_biosses_seed9_debug\n",
      "--------------------------\n",
      "Debugging with only exp n°9\n",
      "['random_25%_biosses_seed1_debug']\n",
      "['python /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/evaluation/run_biosses.py --pearsonr_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/evaluation/metrics/evaluate_pearsonr.py --biosses_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/data/biosses --model_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/random_25% --seed 1 --evaluate_cache_dir evaluation/out/.evaluate_cache/random_25%_biosses_seed1_debug --output_dir /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/evaluation/out/random_25%_biosses_seed1_debug ']\n"
     ]
    }
   ],
   "source": [
    "base_cmd = f\"python {run_biosses_path} \"\n",
    "base_cmd += f\"--pearsonr_path {pearsonr_path} \"\n",
    "base_cmd += f\"--biosses_path {biosses_path} \"\n",
    "# add exps\n",
    "seed_nb = 10\n",
    "cmds, exp_names = [], []\n",
    "for model_path in models_paths:\n",
    "    for seed in range(seed_nb):\n",
    "        cmd = base_cmd\n",
    "        cmd += f\"--model_path {model_path} \"\n",
    "        cmd += f\"--seed {seed} \"\n",
    "        # exp_name\n",
    "        exp_name = f\"{model_path.split('/')[-1]}_biosses_seed{seed}\"\n",
    "        exp_name += '_debug' if debug else ''\n",
    "        cmd += f\"--evaluate_cache_dir evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "        # out_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        cmd += f\"--output_dir {out_dir} \"\n",
    "        # fill lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 9 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)\n",
    "    print(cmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6c1a7ba-c4fb-46f4-aa89-78d7b8ca0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1882820\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"01:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d17c5-7483-43f4-87ac-9f7ae4fa680f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84ad405f-0458-4997-a7a8-def02cad2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "run_hoc_path = f\"{root}/evaluation/run_hoc.py\"\n",
    "f1_path = f\"{root}/evaluation/metrics/evaluate_f1.py\"\n",
    "hoc_path = f\"{root}/data/hallmarks_of_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2637c0a0-73dd-44ff-a267-2379cc67994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_hoc_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--trust_remote_code true \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--load_from_disk \"\n",
    "base_cmd += f\"--dataset_path {hoc_path} \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--metric_path {f1_path} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# add exps\n",
    "seed_nb = 5\n",
    "cmds, exp_names = [], []\n",
    "for model_path in models_paths:\n",
    "    for seed in range(seed_nb):\n",
    "        cmd = base_cmd\n",
    "        cmd += f\"--model_name_or_path {model_path} \"\n",
    "        cmd += f\"--seed {seed} \"\n",
    "        # exp_name\n",
    "        exp_name = f\"{model_path.split('/')[-1]}_hoc_seed{seed}\"\n",
    "        exp_name += '_debug' if debug else ''\n",
    "        cmd += f\"--evaluate_cache_dir evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "        # out_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        cmd += f\"--output_dir {out_dir} \"\n",
    "        # fill lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "            #print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 5 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e29ac59-a740-4b6e-9d50-5705e3037784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941889\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941890\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941891\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941892\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941894\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec6a0d-b11a-432d-b4f6-eda1bf033616",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1e8c461-f68c-431a-b37d-de652c88ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "run_qa_path = f\"{root}/evaluation/run_qa.py\"\n",
    "f1_path = f\"{root}/evaluation/metrics/evaluate_accuracy.py\"\n",
    "datasets_paths = [f\"{root}/data/pubmed_qa\",f\"{root}/data/bioasq_task_b\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5da346ec-7b2a-4f4c-8b2c-84c0d6a1eab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_qa_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--trust_remote_code true \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--text_column_names question,context \"\n",
    "base_cmd += \"--text_column_delimiter [SEP] \"\n",
    "base_cmd += \"--label_column_name answer \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--metric_path {f1_path} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# add exps\n",
    "seed_nb = 10 # 10 RUNS FOR QA DATASETS\n",
    "cmds, exp_names = [], []\n",
    "for dataset_path in datasets_paths:\n",
    "    for model_path in models_paths:\n",
    "        for seed in range(seed_nb):\n",
    "            cmd = base_cmd\n",
    "            cmd += f\"--dataset_path {dataset_path} \"\n",
    "            cmd += f\"--model_name_or_path {model_path} \"\n",
    "            cmd += f\"--seed {seed} \"\n",
    "            # exp_name\n",
    "            exp_name = f\"{model_path.split('/')[-1]}\"\n",
    "            exp_name += f\"_{dataset_path.split('/')[-1].replace('_','-')}_seed{seed}\"\n",
    "            exp_name += '_debug' if debug else ''\n",
    "            cmd += f\"--evaluate_cache_dir evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "            # out_dir\n",
    "            out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "            cmd += f\"--output_dir {out_dir} \"\n",
    "            # fill lists\n",
    "            if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "                #print(exp_name, \"already finished\")\n",
    "                continue\n",
    "            cmds.append(cmd)\n",
    "            exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 39 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ee19836-c281-46c7-8179-752b8925b2e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941896\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941898\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941899\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941900\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941902\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941903\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941905\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941906\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941907\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941909\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941910\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941911\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941913\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941914\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941915\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941917\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941918\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941920\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941921\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941922\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941926\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1941928\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f319a-5be1-4dcd-98d5-4010a219c564",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## rel. extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927e6911-1a6b-4fd3-a53d-d802b57bb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "run_path = f\"{root}/evaluation/run_relation_extraction.py\"\n",
    "metric_path = f\"{root}/evaluation/metrics/evaluate_f1.py\"\n",
    "datasets_paths = [f\"{root}/data/chemprot\",f\"{root}/data/ddi_corpus\",f\"{root}/data/gad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67b0690-6c26-44d8-b26b-1b8add80a0f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--trust_remote_code true \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--text_column_names text \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--metric_path {metric_path} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# add exps\n",
    "seed_nb = 5\n",
    "cmds, exp_names = [], []\n",
    "for dataset_path in datasets_paths:\n",
    "    for model_path in models_paths:\n",
    "        for seed in range(seed_nb):\n",
    "            cmd = base_cmd\n",
    "            cmd += f\"--dataset_path {dataset_path} \"\n",
    "            cmd += f\"--model_name_or_path {model_path} \"\n",
    "            cmd += f\"--seed {seed} \"\n",
    "            # exp_name\n",
    "            exp_name = f\"{model_path.split('/')[-1]}\"\n",
    "            exp_name += f\"_{dataset_path.split('/')[-1].replace('_','-')}_seed{seed}\"\n",
    "            exp_name += '_debug' if debug else ''\n",
    "            cmd += f\"--evaluate_cache_dir evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "            # out_dir\n",
    "            out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "            cmd += f\"--output_dir {out_dir} \"\n",
    "            # fill lists\n",
    "            if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "                #print(exp_name, \"already finished\")\n",
    "                continue\n",
    "            cmds.append(cmd)\n",
    "            exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 94 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c55fd5a-89ec-48cd-8960-676290d5cf2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1942752\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047a6bf-6810-4216-8ee5-1ec3fd85e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# job control and wandb synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a876d489-5b18-48e0-97da-f8b1b004cf41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1977363    gpu_p5 none_all  urz45id  R    1:25:16      1 jean-zay-iam33\n",
      "           1977364    gpu_p5 sjr_top5  urz45id  R    1:25:16      1 jean-zay-iam33\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e366883-5739-46c9-b808-5c25e2885d58",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/debug-cli.urz45id.log\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240514_083902-0mxzed0b/run-0mxzed0b.wandb\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/7w86iyyk ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/koyattah ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/0331ux9l ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/bspo10rb ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/by42bx12 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/evzbde8i ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/5qtivg5l ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/g97tgf7q ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/s7w91w75 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/wgmeffso ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/jkxvyzkj ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/3ccm4km1 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/mpmvz96w ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/zm5yxcbv ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/5voblg5z ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/j1p6wel6 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/pqx1duy1 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/zvfz9fub ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/78zmcxk0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/nf42rusf ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/onyuxcuf ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/heztnkeb ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/fz8yzh4z ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240514_141218-v8abme9d/run-v8abme9d.wandb\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/6h40ksz2 ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/t65uswxu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/316jjsfr ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/tmldqfgo ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/dxiyg835 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/yrtyurfe ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/h7z7xecu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/ojxzxxyy ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/xiqcwusp ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/cg1hyuni ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-classification/runs/t1rh3ggl ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/k9u23goj ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/30fqmzid ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/r4kjwvpe ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/khjjf0zh ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/2p9ac4nh ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/44ilwk6p ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/4f5bcn5l ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/9nguc3ui ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/dq9qfzsb ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/gz6coxe3 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/k6jy4zbw ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/lig42629 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/vc6eu6ps ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/y6p0lvlm ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/gxoi8ntw ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/q2hfvcqo ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/ch0cm6ae ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/d421ksnk ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/jal79f06 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/mg2hgpwb ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/n4s36ksh ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/nbngoaob ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/nka5eyiw ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/i4m59puo ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/qos232ij ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/3ddld0yo ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/ldhjd85h ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/pe415i3s ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/t5vzq9fw ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/au6uc55g ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/gjhdphwu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/ksr5bx1z ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/m6sm0g3k ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/x44t35qw ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/g5609hom ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/x8ogchsk ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/eoz79u2w ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/tz5i3f2x ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/01aitze0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/jv6szm1a ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/ovzwcouv ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/0b1amtsw ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/fzrrhy18 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/lzm53v9h ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/mq4jlkxr ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/35w3bkv2 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/1gowvwur ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/cz9sr82a ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/4rzwlhzx ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/5wernabn ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/aojgm4wc ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/ij6upo39 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/jq3op5x7 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/k7t127a0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/qjuex972 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/4ia7306w ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/yi3v5ib9 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/q9bs1nl8 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/tpnh5ing ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/cm7gv4wu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-qa/runs/nhc3w6q4 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/cozcmot1 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/lobs0yb2 ... done.\n"
     ]
    }
   ],
   "source": [
    "# sync weigths and biases\n",
    "# TODO : handle distributed logging (one wandb run for each GPU used currently)\n",
    "!wandb sync --include-offline wandb/offline-* --clean-force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f640be-90f6-46b7-a8a3-ee3b078f5f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9cb63c1-4941-4a76-be5f-6f2bebf4b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel all my jobs\n",
    "!scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6f387b2-27ac-4919-a0b3-66693ff420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete logs\n",
    "!rm -rf slurm/log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e795ecc8-a51d-402f-8366-876311b3b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete debug dirs\n",
    "!rm -rf pretraining/*_debug/\n",
    "!rm -rf pretraining/*_ckpt-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebfae16-93c2-404a-b61e-018e7e295b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf evaluation/out/*_debug\n",
    "!rm -rf evaluation/out/*_ckpt-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd51326a-116b-452b-973e-a73eb55a3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete wandb run dir\n",
    "!rm -rf wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab423f1a-210c-47c2-bc01-8c36c94b0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete evaluate cache\n",
    "!rm -rf evaluation/out/.evaluate_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b414574-1b8e-4c1c-93d3-ffef13acfb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete slurm files\n",
    "!rm -rf slurm/*.slurm\n",
    "!rm -rf data/.blurb_cache/seqeval\n",
    "!rm -rf core-python-*\n",
    "!rm -rf **/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb05bcc1-10d8-40c0-a4bd-64b26507f1a5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy eval output dirs to WORK\n",
    "out_dir = f\"{os.getenv('WORK')}/results/pretrain-med-data-qual\"\n",
    "for pred_f in glob.glob(\"evaluation/out/*/predict_results.json\"):\n",
    "    out_f = f\"{out_dir}/{pred_f.split('/')[-2]}.json\"\n",
    "    if not os.path.exists(out_f):\n",
    "        res = json.load(open(pred_f))\n",
    "        json.dump(res,open(out_f,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d8b10f-ad8f-48dc-bcd0-9ddfe343b7cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean evaluation output dirs\n",
    "for res_dir in glob.glob(\"evaluation/out/*/\"):\n",
    "    if os.path.exists(os.path.join(res_dir,\"predict_results.json\")):\n",
    "        for ckpt in glob.glob(f\"{res_dir}/checkpoint-*\"):\n",
    "            shutil.rmtree(ckpt)\n",
    "        for fpath in glob.glob(f\"{res_dir}/*.*\"):\n",
    "            if \"result\" not in fpath:\n",
    "                os.remove(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "197715c6-a656-41ad-b341-0f986ca6c80a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy models to WORK\n",
    "models_work = os.path.join(os.getenv(\"WORK\"),\"models\",\"perso\")\n",
    "for subp in os.listdir(\"pretraining\"):\n",
    "    if '.' in subp:continue\n",
    "    renamed = \"bert-bio_\" + subp.replace(\"_\",\"-\").replace(\"%\",\"\").replace(\"h-index\",\"hind\")\n",
    "    out_dir = os.path.join(models_work,renamed)\n",
    "    if os.path.exists(f\"pretraining/{subp}/model.safetensors\"):\n",
    "        if not os.path.isdir(out_dir):os.mkdir(out_dir)\n",
    "        for f in os.listdir(f\"pretraining/{subp}\"):\n",
    "            if \"checkpoint\" not in f:\n",
    "                shutil.copy(f\"pretraining/{subp}/{f}\",out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.2.0_py3.11.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.2.0_py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
