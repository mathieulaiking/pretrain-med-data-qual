{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e7beae-3b06-4282-93e3-b9c7ac918ac6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "from idr_pytools import gpu_jobs_submitter\n",
    "\n",
    "# project root path\n",
    "dsdir = os.getenv(\"DSDIR\")\n",
    "scratch = os.getenv(\"SCRATCH\")\n",
    "root = os.path.join(scratch,\"pretrain-med-data-qual\")\n",
    "idr_models_dir = os.path.join(dsdir,\"HuggingFace_Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaaa70-65b5-4f87-8983-4f39f5f2a0bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# bert mlm pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8f86e-be94-4339-8be5-d7dc5ffd58fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## computing number of optimal steps and grad accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171c5ae-b85e-4a3e-8db0-9d2a1d2a1300",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define the number of optimal steps as the number of steps required to perform an entire epoch on the full PubMed dataset (Baseline last update january 2024).\n",
    "Following RoBERTa, we aim for an effective batch_size of 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efafc7-412f-4796-a8b4-bb2fee4e2710",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Pretraining Phases Hardware arguments are taken from [NVIDIA Pytorch BERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#pre-training-nvidia-dgx-a100-8x-a100-80gb) and [NVIDIA Tensorflow BioBERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/biobert/README.md#pre-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e078cec7-86cb-4b17-8beb-6ee86920490b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation needed for effective batch size of 8192, with 2*a100 GPUs with per device batch size of 32 =  128\n",
      "Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : 4194304\n",
      "Optimal step number with effective batch size of 8192 : 3598.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient accumulation\n",
    "sequence_length = 512\n",
    "gpu_model = \"a100\"\n",
    "max_batch_size_per_gpu = 32 # on A100 with 512 seq length\n",
    "gpu_nb = 2\n",
    "target_batch_size = 8192\n",
    "gradient_accumulation = target_batch_size // (gpu_nb*max_batch_size_per_gpu) \n",
    "print(f\"Gradient accumulation needed for effective batch size of {target_batch_size}, with {gpu_nb}*{gpu_model} GPUs with per device batch size of {max_batch_size_per_gpu} = \",gradient_accumulation)\n",
    "# Optimal steps number\n",
    "total_token_nb =  15888466068 # calculated number of tokens in pubmed\n",
    "train_token_nb = 0.95*total_token_nb\n",
    "token_per_step = target_batch_size * sequence_length\n",
    "optimal_train_step_nb = train_token_nb // token_per_step\n",
    "print(f\"Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : {token_per_step}\")\n",
    "print(f\"Optimal step number with effective batch size of {target_batch_size} : {optimal_train_step_nb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bec35-8863-41ef-a311-343f4f1b1d7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## defining arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e357672-8722-46ea-bf07-2a404a8aab0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "pubmed_path = f\"{root}/data/pubmed_preproc\"\n",
    "bert_path = f\"{idr_models_dir}/bert-base-uncased\"\n",
    "run_mlm_path = f\"{root}/pretraining/run_mlm_offline.py \"\n",
    "accuracy_path = f\"{root}/pretraining/accuracy.py\"\n",
    "out_dir_template = f\"{root}/pretraining/{{exp_name}}\"\n",
    "\n",
    "# Pretraining Phases Hardware Arguments for A100 GPUs\n",
    "n_gpu = 2\n",
    "sequence_length = 512\n",
    "batch_size = 32 # per device\n",
    "precision = \"fp16\"\n",
    "max_steps = 3598 if not debug else 100\n",
    "acc_steps = 128 if not debug else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b4ae9a2-64dd-485c-82c3-6a7303cfc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none_all_debug run train and eval\n",
      "random_50%_debug run train and eval\n",
      "random_25%_debug run train and eval\n",
      "h-index_top50%_debug run train and eval\n",
      "h-index_mid50%_debug run train and eval\n",
      "h-index_top25%_debug run train and eval\n",
      "h-index_mid25%_debug run train and eval\n",
      "sjr_top25%_debug run train and eval\n",
      "sjr_mid25%_debug run train and eval\n",
      "sjr_top50%_debug run train and eval\n",
      "0 : none_all_debug\n",
      "1 : random_50%_debug\n",
      "2 : random_25%_debug\n",
      "3 : h-index_top50%_debug\n",
      "4 : h-index_mid50%_debug\n",
      "5 : h-index_top25%_debug\n",
      "6 : h-index_mid25%_debug\n",
      "7 : sjr_top25%_debug\n",
      "8 : sjr_mid25%_debug\n",
      "9 : sjr_top50%_debug\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = \"torchrun --standalone \"\n",
    "base_cmd += f\"--nproc_per_node {n_gpu} \"\n",
    "base_cmd += \"--nnodes 1 \"\n",
    "base_cmd += f\"{run_mlm_path} \"\n",
    "\n",
    "# Model Arguments\n",
    "base_cmd += f\"--model_name_or_path {bert_path} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += f\"--dataset_name {pubmed_path} \"\n",
    "base_cmd += f\"--metric_path {accuracy_path} \"\n",
    "base_cmd += f\"--max_eval_samples {25600} \" if debug else \"\"\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {sequence_length} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--seed 42 \" \n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## BERT hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 1e-4 \" \n",
    "base_cmd += \"--weight_decay 0.01 \" \n",
    "base_cmd += \"--adam_beta1 0.9 \" \n",
    "base_cmd += \"--adam_beta2 0.999 \" \n",
    "base_cmd += \"--adam_epsilon 1e-6 \" # RoBERTa\n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "base_cmd += f\"--gradient_accumulation_steps {acc_steps} \" if acc_steps else \"\"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--max_steps {max_steps} \"\n",
    "base_cmd += f\"--warmup_steps {max_steps//10} \" # warmup for 10% of steps\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy no \" # no evaluation during training only at the end for perplexity\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--logging_steps 0.01 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--log_on_each_node false \"\n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Data Filters experiences\n",
    "bounds = {\n",
    "    \"none\":[\n",
    "        (None,None,\"all\"),\n",
    "    ],\n",
    "    \"random\":[\n",
    "        (0.0,0.5,\"50%\"),\n",
    "        (0.0,0.25,\"25%\")\n",
    "    ],\n",
    "    \"h-index\":[\n",
    "        (103,1400,\"top50%\"),\n",
    "        (53,190,\"mid50%\"),\n",
    "        (190,1400,\"top25%\"),\n",
    "        (77,142,\"mid25%\"),\n",
    "    ],\n",
    "    \"sjr\":[\n",
    "        (1.312,100.0,\"top25%\"),\n",
    "        (0.462,0.984,\"mid25%\"),\n",
    "        (0.759,100.0,\"top50%\"),\n",
    "    ]\n",
    "}\n",
    "cmds = []\n",
    "exp_names = []\n",
    "for metric, exps in bounds.items():\n",
    "    for lower_bound, upper_bound, bound_name in exps:\n",
    "        cmd = base_cmd\n",
    "        # filtering metric\n",
    "        if metric != \"none\": \n",
    "            cmd += f\"--filter_metric {metric} \"\n",
    "            cmd += f\"--filter_lower_threshold {lower_bound} \"\n",
    "            cmd += f\"--filter_upper_threshold {upper_bound} \"\n",
    "        else:\n",
    "            cmd += \"--streaming \"\n",
    "        # experience name\n",
    "        exp_name = f\"{metric}_{bound_name}\" \n",
    "        if debug : exp_name += \"_debug\"\n",
    "        # evaluate cache dir\n",
    "        cmd += f\"--evaluate_cache_dir pretraining/.evaluate_cache/{exp_name} \"\n",
    "        # output_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        cmd += f\"--output_dir {out_dir} \"\n",
    "        # wandb args\n",
    "        cmd += f\"--wandb_group {exp_name}_{gpu}x{n_gpu} \"\n",
    "        cmd += f\"--wandb_name {exp_name} \"\n",
    "        # append to lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"eval_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        elif os.path.exists(os.path.join(out_dir,\"train_results.json\")):\n",
    "            print(exp_name,\"run eval only\")\n",
    "            cmd += \"--do_eval \"  \n",
    "        else : \n",
    "            print(exp_name,\"run train and eval\")\n",
    "            cmd += \"--do_train \" \n",
    "            cmd += \"--do_eval \" \n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "        \n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc582b13-fa85-4d71-a10f-8b662dcde050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['torchrun --standalone --nproc_per_node 2 --nnodes 1 /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/run_mlm_offline.py  --model_name_or_path /gpfsdswork/dataset/HuggingFace_Models/bert-base-uncased --dataset_name /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/data/pubmed_preproc --metric_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/accuracy.py --max_eval_samples 25600 --preprocessing_num_workers 8 --max_seq_length 512 --seed 42 --overwrite_output_dir true --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 1e-4 --weight_decay 0.01 --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-6 --fp16 true --eval_accumulation_steps 2 --gradient_accumulation_steps 2 --max_steps 100 --warmup_steps 10 --evaluation_strategy no --logging_strategy steps --save_strategy steps --logging_steps 0.01 --save_steps 0.1 --logging_first_step true --log_on_each_node false --save_total_limit 2 --disable_tqdm true --report_to wandb --filter_metric h-index --filter_lower_threshold 53 --filter_upper_threshold 190 --evaluate_cache_dir pretraining/.evaluate_cache/h-index_mid50%_debug --output_dir /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/h-index_mid50%_debug --wandb_group h-index_mid50%_debug_a100x2 --wandb_name h-index_mid50%_debug --do_train --do_eval ']\n",
      "['h-index_mid50%_debug']\n"
     ]
    }
   ],
   "source": [
    "if debug : \n",
    "    debug_ind = 4\n",
    "    cmds = [cmds[debug_ind]]\n",
    "    exp_names = [exp_names[debug_ind]]\n",
    "    print(cmds)\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d07e-4f5e-47ce-91be-b917ce65a387",
   "metadata": {
    "tags": []
   },
   "source": [
    "## launching jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11c382f9-5cb3-431c-b57b-a95ac5e5b2cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1882016\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :\n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"a100\",\n",
    "        time_max=\"20:00:00\" if not debug else \"1:00:00\",\n",
    "        account=f\"aro@a100\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd455b5-6404-4ef1-bde8-581b69616f1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# fine-tune blurb eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cb07337-e312-45e2-bdb6-a1eb7d200ad8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = f\"{root}/data/.blurb_cache\"\n",
    "out_dir_template = f\"{root}/evaluation/out/{{exp_name}}\"\n",
    "models_paths = [\n",
    "    f\"{idr_models_dir}/bert-base-uncased\",\n",
    "    f\"{root}/pretraining/random_25%\",\n",
    "    f\"{root}/pretraining/h-index_mid25%\",\n",
    "    f\"{root}/pretraining/sjr_top25%\",\n",
    "    f\"{root}/pretraining/sjr_mid25%\",\n",
    "    f\"{root}/pretraining/h-index_mid50%\",\n",
    "    f\"{root}/pretraining/h-index_top25%\",\n",
    "]\n",
    "\n",
    "# Pretraining Phases Hardware Arguments \n",
    "# for 1xV100\n",
    "max_seq_length = 512\n",
    "batch_size = 16 # per device\n",
    "precision = \"fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69123ad3-bd4b-4e9f-8a05-76e622224901",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808de476-3a97-4ad5-8c5c-8d93eea067ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "run_ner_path = f\"{root}/evaluation/run_ner.py \"\n",
    "seqeval_path = f\"{root}/evaluation/metrics/evaluate_seqeval.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cbce7e-4a37-4f76-818d-37456feb9493",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_blurb-bc5chem_seed0 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed1 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed2 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed3 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed4 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed0 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed1 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed2 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed3 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed4 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed0 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed1 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed2 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed3 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed4 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed0 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed1 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed2 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed3 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed4 already finished\n",
      "bert-base-uncased_blurb-ncbi_disease_seed0 already finished\n",
      "bert-base-uncased_blurb-ncbi_disease_seed1 already finished\n",
      "bert-base-uncased_blurb-ncbi_disease_seed2 already finished\n",
      "bert-base-uncased_blurb-ncbi_disease_seed3 already finished\n",
      "bert-base-uncased_blurb-ncbi_disease_seed4 already finished\n",
      "random_25%_blurb-bc5chem_seed0 already finished\n",
      "random_25%_blurb-bc5chem_seed1 already finished\n",
      "random_25%_blurb-bc5chem_seed2 already finished\n",
      "random_25%_blurb-bc5chem_seed3 already finished\n",
      "random_25%_blurb-bc5chem_seed4 already finished\n",
      "random_25%_blurb-bc5disease_seed0 already finished\n",
      "random_25%_blurb-bc5disease_seed1 already finished\n",
      "random_25%_blurb-bc5disease_seed2 already finished\n",
      "random_25%_blurb-bc5disease_seed3 already finished\n",
      "random_25%_blurb-bc5disease_seed4 already finished\n",
      "random_25%_blurb-bc2gm_seed0 already finished\n",
      "random_25%_blurb-bc2gm_seed1 already finished\n",
      "random_25%_blurb-bc2gm_seed2 already finished\n",
      "random_25%_blurb-bc2gm_seed3 already finished\n",
      "random_25%_blurb-bc2gm_seed4 already finished\n",
      "random_25%_blurb-jnlpba_seed0 already finished\n",
      "random_25%_blurb-jnlpba_seed1 already finished\n",
      "random_25%_blurb-jnlpba_seed2 already finished\n",
      "random_25%_blurb-jnlpba_seed3 already finished\n",
      "random_25%_blurb-jnlpba_seed4 already finished\n",
      "random_25%_blurb-ncbi_disease_seed0 already finished\n",
      "random_25%_blurb-ncbi_disease_seed1 already finished\n",
      "random_25%_blurb-ncbi_disease_seed2 already finished\n",
      "random_25%_blurb-ncbi_disease_seed3 already finished\n",
      "random_25%_blurb-ncbi_disease_seed4 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed0 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed1 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed2 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed3 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed4 already finished\n",
      "h-index_mid25%_blurb-bc5disease_seed0 already finished\n",
      "h-index_mid25%_blurb-bc5disease_seed1 already finished\n",
      "h-index_mid25%_blurb-bc5disease_seed2 already finished\n",
      "h-index_mid25%_blurb-bc5disease_seed3 already finished\n",
      "h-index_mid25%_blurb-bc5disease_seed4 already finished\n",
      "h-index_mid25%_blurb-bc2gm_seed0 already finished\n",
      "h-index_mid25%_blurb-bc2gm_seed1 already finished\n",
      "h-index_mid25%_blurb-bc2gm_seed2 already finished\n",
      "h-index_mid25%_blurb-bc2gm_seed3 already finished\n",
      "h-index_mid25%_blurb-bc2gm_seed4 already finished\n",
      "h-index_mid25%_blurb-jnlpba_seed0 already finished\n",
      "h-index_mid25%_blurb-jnlpba_seed1 already finished\n",
      "h-index_mid25%_blurb-jnlpba_seed2 already finished\n",
      "h-index_mid25%_blurb-jnlpba_seed3 already finished\n",
      "h-index_mid25%_blurb-jnlpba_seed4 already finished\n",
      "h-index_mid25%_blurb-ncbi_disease_seed0 already finished\n",
      "h-index_mid25%_blurb-ncbi_disease_seed1 already finished\n",
      "h-index_mid25%_blurb-ncbi_disease_seed2 already finished\n",
      "h-index_mid25%_blurb-ncbi_disease_seed3 already finished\n",
      "h-index_mid25%_blurb-ncbi_disease_seed4 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed0 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed1 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed2 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed3 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed4 already finished\n",
      "sjr_top25%_blurb-bc5disease_seed0 already finished\n",
      "sjr_top25%_blurb-bc5disease_seed1 already finished\n",
      "sjr_top25%_blurb-bc5disease_seed2 already finished\n",
      "sjr_top25%_blurb-bc5disease_seed3 already finished\n",
      "sjr_top25%_blurb-bc5disease_seed4 already finished\n",
      "sjr_top25%_blurb-bc2gm_seed0 already finished\n",
      "sjr_top25%_blurb-bc2gm_seed1 already finished\n",
      "sjr_top25%_blurb-bc2gm_seed2 already finished\n",
      "sjr_top25%_blurb-bc2gm_seed3 already finished\n",
      "sjr_top25%_blurb-bc2gm_seed4 already finished\n",
      "sjr_top25%_blurb-jnlpba_seed0 already finished\n",
      "sjr_top25%_blurb-jnlpba_seed1 already finished\n",
      "sjr_top25%_blurb-jnlpba_seed2 already finished\n",
      "sjr_top25%_blurb-jnlpba_seed3 already finished\n",
      "sjr_top25%_blurb-jnlpba_seed4 already finished\n",
      "sjr_top25%_blurb-ncbi_disease_seed0 already finished\n",
      "sjr_top25%_blurb-ncbi_disease_seed1 already finished\n",
      "sjr_top25%_blurb-ncbi_disease_seed2 already finished\n",
      "sjr_top25%_blurb-ncbi_disease_seed3 already finished\n",
      "sjr_top25%_blurb-ncbi_disease_seed4 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed0 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed1 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed2 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed3 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed4 already finished\n",
      "sjr_mid25%_blurb-bc5disease_seed0 already finished\n",
      "sjr_mid25%_blurb-bc5disease_seed1 already finished\n",
      "sjr_mid25%_blurb-bc5disease_seed2 already finished\n",
      "sjr_mid25%_blurb-bc5disease_seed3 already finished\n",
      "sjr_mid25%_blurb-bc5disease_seed4 already finished\n",
      "sjr_mid25%_blurb-bc2gm_seed0 already finished\n",
      "sjr_mid25%_blurb-bc2gm_seed1 already finished\n",
      "sjr_mid25%_blurb-bc2gm_seed2 already finished\n",
      "sjr_mid25%_blurb-bc2gm_seed3 already finished\n",
      "sjr_mid25%_blurb-bc2gm_seed4 already finished\n",
      "sjr_mid25%_blurb-jnlpba_seed0 already finished\n",
      "sjr_mid25%_blurb-jnlpba_seed1 already finished\n",
      "sjr_mid25%_blurb-jnlpba_seed2 already finished\n",
      "sjr_mid25%_blurb-jnlpba_seed3 already finished\n",
      "sjr_mid25%_blurb-jnlpba_seed4 already finished\n",
      "sjr_mid25%_blurb-ncbi_disease_seed0 already finished\n",
      "sjr_mid25%_blurb-ncbi_disease_seed1 already finished\n",
      "sjr_mid25%_blurb-ncbi_disease_seed2 already finished\n",
      "sjr_mid25%_blurb-ncbi_disease_seed3 already finished\n",
      "sjr_mid25%_blurb-ncbi_disease_seed4 already finished\n",
      "h-index_mid50%_blurb-bc5chem_seed0 already finished\n",
      "h-index_mid50%_blurb-bc5chem_seed1 already finished\n",
      "h-index_mid50%_blurb-bc5chem_seed2 already finished\n",
      "h-index_mid50%_blurb-bc5chem_seed3 already finished\n",
      "h-index_mid50%_blurb-bc5chem_seed4 already finished\n",
      "h-index_mid50%_blurb-bc5disease_seed0 already finished\n",
      "h-index_mid50%_blurb-bc5disease_seed1 already finished\n",
      "h-index_mid50%_blurb-bc5disease_seed2 already finished\n",
      "h-index_mid50%_blurb-bc5disease_seed3 already finished\n",
      "h-index_mid50%_blurb-bc5disease_seed4 already finished\n",
      "h-index_mid50%_blurb-bc2gm_seed0 already finished\n",
      "h-index_mid50%_blurb-bc2gm_seed1 already finished\n",
      "h-index_mid50%_blurb-bc2gm_seed2 already finished\n",
      "h-index_mid50%_blurb-bc2gm_seed3 already finished\n",
      "h-index_mid50%_blurb-bc2gm_seed4 already finished\n",
      "h-index_mid50%_blurb-jnlpba_seed0 already finished\n",
      "h-index_mid50%_blurb-jnlpba_seed1 already finished\n",
      "h-index_mid50%_blurb-jnlpba_seed2 already finished\n",
      "h-index_mid50%_blurb-jnlpba_seed3 already finished\n",
      "h-index_mid50%_blurb-jnlpba_seed4 already finished\n",
      "h-index_mid50%_blurb-ncbi_disease_seed0 already finished\n",
      "h-index_mid50%_blurb-ncbi_disease_seed1 already finished\n",
      "h-index_mid50%_blurb-ncbi_disease_seed2 already finished\n",
      "h-index_mid50%_blurb-ncbi_disease_seed3 already finished\n",
      "h-index_mid50%_blurb-ncbi_disease_seed4 already finished\n",
      "h-index_top25%_blurb-bc5chem_seed0 already finished\n",
      "h-index_top25%_blurb-bc5chem_seed1 already finished\n",
      "h-index_top25%_blurb-bc5chem_seed2 already finished\n",
      "h-index_top25%_blurb-bc5chem_seed3 already finished\n",
      "h-index_top25%_blurb-bc5chem_seed4 already finished\n",
      "h-index_top25%_blurb-bc5disease_seed0 already finished\n",
      "h-index_top25%_blurb-bc5disease_seed1 already finished\n",
      "h-index_top25%_blurb-bc5disease_seed2 already finished\n",
      "h-index_top25%_blurb-bc5disease_seed3 already finished\n",
      "h-index_top25%_blurb-bc5disease_seed4 already finished\n",
      "h-index_top25%_blurb-bc2gm_seed0 already finished\n",
      "h-index_top25%_blurb-bc2gm_seed1 already finished\n",
      "h-index_top25%_blurb-bc2gm_seed2 already finished\n",
      "h-index_top25%_blurb-bc2gm_seed3 already finished\n",
      "h-index_top25%_blurb-bc2gm_seed4 already finished\n",
      "h-index_top25%_blurb-jnlpba_seed0 already finished\n",
      "h-index_top25%_blurb-jnlpba_seed1 already finished\n",
      "h-index_top25%_blurb-jnlpba_seed2 already finished\n",
      "h-index_top25%_blurb-jnlpba_seed3 already finished\n",
      "h-index_top25%_blurb-jnlpba_seed4 already finished\n",
      "h-index_top25%_blurb-ncbi_disease_seed0 already finished\n",
      "h-index_top25%_blurb-ncbi_disease_seed1 already finished\n",
      "h-index_top25%_blurb-ncbi_disease_seed2 already finished\n",
      "h-index_top25%_blurb-ncbi_disease_seed3 already finished\n",
      "h-index_top25%_blurb-ncbi_disease_seed4 already finished\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_ner_path} \"\n",
    "\n",
    "# Model args\n",
    "base_cmd += f\"--cache_dir {cache_dir} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--seqeval_path {seqeval_path} \"\n",
    "base_cmd += \"--return_entity_level_metrics \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Different experiments Runs \n",
    "datasets_configs=[\n",
    "    (\"bigbio/blurb\",\"bc5chem\"),\n",
    "    (\"bigbio/blurb\",\"bc5disease\"),\n",
    "    (\"bigbio/blurb\",\"bc2gm\"),\n",
    "    (\"bigbio/blurb\",\"jnlpba\"),\n",
    "    (\"bigbio/blurb\",\"ncbi_disease\"),\n",
    "]\n",
    "seed_nb = 5\n",
    "\n",
    "cmds = []\n",
    "exp_names = []\n",
    "\n",
    "for model_path in models_paths :\n",
    "    for dataset_name, dataset_config in datasets_configs :\n",
    "        for seed in range(seed_nb):\n",
    "            cmd = base_cmd\n",
    "            cmd += f\"--dataset_name {dataset_name} \"\n",
    "            cmd += f\"--dataset_config_name {dataset_config} \" if dataset_config else \"\"\n",
    "            cmd += f\"--seed {seed} \"\n",
    "            cmd += f\"--model_name_or_path {model_path} \"\n",
    "            \n",
    "            # Experience name for output directory\n",
    "            exp_name = f\"{model_path.split('/')[-1]}_{dataset_name.split('/')[-1]}\"\n",
    "            exp_name += f\"-{dataset_config}\" if dataset_config else \"\"\n",
    "            exp_name += f\"_seed{seed}\"\n",
    "            exp_name += \"_debug\" if debug else \"\"\n",
    "            out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "            cmd += f\"--output_dir {out_dir} \"\n",
    "            # Cache dir (for cache problems)\n",
    "            base_cmd += f\"--evaluate_cache_dir {root}/evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "            # Weights and Biases\n",
    "            cmd += f\"--run_name {exp_name}\"\n",
    "            # fill lists\n",
    "            if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "                print(exp_name, \"already finished\")\n",
    "                continue\n",
    "            cmds.append(cmd)\n",
    "            exp_names.append(exp_name)\n",
    "\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 24 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af2d0d3d-424b-4694-9710-b81949dd93e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865736\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865738\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865739\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865740\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865741\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865743\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865745\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865746\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865747\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865749\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865750\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865753\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865754\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865756\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865757\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865758\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865760\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865763\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865765\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865766\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865768\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865771\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865772\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865773\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865774\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865775\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865777\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865778\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865779\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865781\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865782\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865784\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865785\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865786\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865787\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865788\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865789\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865791\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865793\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865794\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865795\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865796\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865797\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865798\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865799\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865801\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865802\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865803\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865804\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865805\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865806\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865807\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865808\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865809\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865811\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865813\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865814\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865815\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865816\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865818\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865819\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865820\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865821\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865822\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865823\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865824\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865825\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865826\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865827\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865828\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865830\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865831\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865832\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865833\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865835\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865836\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865837\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865838\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865840\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865841\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865842\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865843\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865844\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865846\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865847\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865848\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865851\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865853\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865854\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865855\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865856\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865857\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865859\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865860\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865861\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865862\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865863\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865864\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865865\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865866\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865867\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865869\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865870\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865871\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865872\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865873\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1865874\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = f\"\"\"module load python/3.11.5\n",
    "conda activate transformers_latest\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cc9b0-3a23-4d02-a369-9dcde2f7662d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## biosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2736f356-2270-4810-a98c-ecdf862267e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "run_biosses_path = f\"{root}/evaluation/run_biosses.py\"\n",
    "biosses_path = f\"{root}/data/biosses\"\n",
    "pearsonr_path = f\"{root}/evaluation/metrics/evaluate_pearsonr.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63c27598-9343-45f4-9a48-ecec9ae41379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_biosses_seed0_debug already finished\n",
      "random_25%_biosses_seed0_debug already finished\n",
      "0 : bert-base-uncased_biosses_seed1_debug\n",
      "1 : bert-base-uncased_biosses_seed2_debug\n",
      "2 : bert-base-uncased_biosses_seed3_debug\n",
      "3 : bert-base-uncased_biosses_seed4_debug\n",
      "4 : bert-base-uncased_biosses_seed5_debug\n",
      "5 : bert-base-uncased_biosses_seed6_debug\n",
      "6 : bert-base-uncased_biosses_seed7_debug\n",
      "7 : bert-base-uncased_biosses_seed8_debug\n",
      "8 : bert-base-uncased_biosses_seed9_debug\n",
      "9 : random_25%_biosses_seed1_debug\n",
      "10 : random_25%_biosses_seed2_debug\n",
      "11 : random_25%_biosses_seed3_debug\n",
      "12 : random_25%_biosses_seed4_debug\n",
      "13 : random_25%_biosses_seed5_debug\n",
      "14 : random_25%_biosses_seed6_debug\n",
      "15 : random_25%_biosses_seed7_debug\n",
      "16 : random_25%_biosses_seed8_debug\n",
      "17 : random_25%_biosses_seed9_debug\n",
      "18 : h-index_mid25%_biosses_seed0_debug\n",
      "19 : h-index_mid25%_biosses_seed1_debug\n",
      "20 : h-index_mid25%_biosses_seed2_debug\n",
      "21 : h-index_mid25%_biosses_seed3_debug\n",
      "22 : h-index_mid25%_biosses_seed4_debug\n",
      "23 : h-index_mid25%_biosses_seed5_debug\n",
      "24 : h-index_mid25%_biosses_seed6_debug\n",
      "25 : h-index_mid25%_biosses_seed7_debug\n",
      "26 : h-index_mid25%_biosses_seed8_debug\n",
      "27 : h-index_mid25%_biosses_seed9_debug\n",
      "28 : sjr_top25%_biosses_seed0_debug\n",
      "29 : sjr_top25%_biosses_seed1_debug\n",
      "30 : sjr_top25%_biosses_seed2_debug\n",
      "31 : sjr_top25%_biosses_seed3_debug\n",
      "32 : sjr_top25%_biosses_seed4_debug\n",
      "33 : sjr_top25%_biosses_seed5_debug\n",
      "34 : sjr_top25%_biosses_seed6_debug\n",
      "35 : sjr_top25%_biosses_seed7_debug\n",
      "36 : sjr_top25%_biosses_seed8_debug\n",
      "37 : sjr_top25%_biosses_seed9_debug\n",
      "38 : sjr_mid25%_biosses_seed0_debug\n",
      "39 : sjr_mid25%_biosses_seed1_debug\n",
      "40 : sjr_mid25%_biosses_seed2_debug\n",
      "41 : sjr_mid25%_biosses_seed3_debug\n",
      "42 : sjr_mid25%_biosses_seed4_debug\n",
      "43 : sjr_mid25%_biosses_seed5_debug\n",
      "44 : sjr_mid25%_biosses_seed6_debug\n",
      "45 : sjr_mid25%_biosses_seed7_debug\n",
      "46 : sjr_mid25%_biosses_seed8_debug\n",
      "47 : sjr_mid25%_biosses_seed9_debug\n",
      "48 : h-index_mid50%_biosses_seed0_debug\n",
      "49 : h-index_mid50%_biosses_seed1_debug\n",
      "50 : h-index_mid50%_biosses_seed2_debug\n",
      "51 : h-index_mid50%_biosses_seed3_debug\n",
      "52 : h-index_mid50%_biosses_seed4_debug\n",
      "53 : h-index_mid50%_biosses_seed5_debug\n",
      "54 : h-index_mid50%_biosses_seed6_debug\n",
      "55 : h-index_mid50%_biosses_seed7_debug\n",
      "56 : h-index_mid50%_biosses_seed8_debug\n",
      "57 : h-index_mid50%_biosses_seed9_debug\n",
      "58 : h-index_top25%_biosses_seed0_debug\n",
      "59 : h-index_top25%_biosses_seed1_debug\n",
      "60 : h-index_top25%_biosses_seed2_debug\n",
      "61 : h-index_top25%_biosses_seed3_debug\n",
      "62 : h-index_top25%_biosses_seed4_debug\n",
      "63 : h-index_top25%_biosses_seed5_debug\n",
      "64 : h-index_top25%_biosses_seed6_debug\n",
      "65 : h-index_top25%_biosses_seed7_debug\n",
      "66 : h-index_top25%_biosses_seed8_debug\n",
      "67 : h-index_top25%_biosses_seed9_debug\n",
      "--------------------------\n",
      "Debugging with only exp n°9\n",
      "['random_25%_biosses_seed1_debug']\n",
      "['python /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/evaluation/run_biosses.py --pearsonr_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/evaluation/metrics/evaluate_pearsonr.py --biosses_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/data/biosses --model_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/random_25% --seed 1 --evaluate_cache_dir evaluation/out/.evaluate_cache/random_25%_biosses_seed1_debug --output_dir /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/evaluation/out/random_25%_biosses_seed1_debug ']\n"
     ]
    }
   ],
   "source": [
    "base_cmd = f\"python {run_biosses_path} \"\n",
    "base_cmd += f\"--pearsonr_path {pearsonr_path} \"\n",
    "base_cmd += f\"--biosses_path {biosses_path} \"\n",
    "# add exps\n",
    "seed_nb = 10\n",
    "cmds, exp_names = [], []\n",
    "for model_path in models_paths:\n",
    "    for seed in range(seed_nb):\n",
    "        cmd = base_cmd\n",
    "        cmd += f\"--model_path {model_path} \"\n",
    "        cmd += f\"--seed {seed} \"\n",
    "        # exp_name\n",
    "        exp_name = f\"{model_path.split('/')[-1]}_biosses_seed{seed}\"\n",
    "        exp_name += '_debug' if debug else ''\n",
    "        cmd += f\"--evaluate_cache_dir evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "        # out_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        cmd += f\"--output_dir {out_dir} \"\n",
    "        # fill lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 9 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)\n",
    "    print(cmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6c1a7ba-c4fb-46f4-aa89-78d7b8ca0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1882820\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"01:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d17c5-7483-43f4-87ac-9f7ae4fa680f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84ad405f-0458-4997-a7a8-def02cad2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "run_hoc_path = f\"{root}/evaluation/run_hoc.py\"\n",
    "f1_path = f\"{root}/evaluation/metrics/evaluate_f1.py\"\n",
    "hoc_path = f\"{root}/data/hallmarks_of_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2637c0a0-73dd-44ff-a267-2379cc67994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_hoc_seed0 already finished\n",
      "bert-base-uncased_hoc_seed1 already finished\n",
      "bert-base-uncased_hoc_seed2 already finished\n",
      "bert-base-uncased_hoc_seed3 already finished\n",
      "bert-base-uncased_hoc_seed4 already finished\n",
      "0 : random_25%_hoc_seed0\n",
      "1 : random_25%_hoc_seed1\n",
      "2 : random_25%_hoc_seed2\n",
      "3 : random_25%_hoc_seed3\n",
      "4 : random_25%_hoc_seed4\n",
      "5 : h-index_mid25%_hoc_seed0\n",
      "6 : h-index_mid25%_hoc_seed1\n",
      "7 : h-index_mid25%_hoc_seed2\n",
      "8 : h-index_mid25%_hoc_seed3\n",
      "9 : h-index_mid25%_hoc_seed4\n",
      "10 : sjr_top25%_hoc_seed0\n",
      "11 : sjr_top25%_hoc_seed1\n",
      "12 : sjr_top25%_hoc_seed2\n",
      "13 : sjr_top25%_hoc_seed3\n",
      "14 : sjr_top25%_hoc_seed4\n",
      "15 : sjr_mid25%_hoc_seed0\n",
      "16 : sjr_mid25%_hoc_seed1\n",
      "17 : sjr_mid25%_hoc_seed2\n",
      "18 : sjr_mid25%_hoc_seed3\n",
      "19 : sjr_mid25%_hoc_seed4\n",
      "20 : h-index_mid50%_hoc_seed0\n",
      "21 : h-index_mid50%_hoc_seed1\n",
      "22 : h-index_mid50%_hoc_seed2\n",
      "23 : h-index_mid50%_hoc_seed3\n",
      "24 : h-index_mid50%_hoc_seed4\n",
      "25 : h-index_top25%_hoc_seed0\n",
      "26 : h-index_top25%_hoc_seed1\n",
      "27 : h-index_top25%_hoc_seed2\n",
      "28 : h-index_top25%_hoc_seed3\n",
      "29 : h-index_top25%_hoc_seed4\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_hoc_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--trust_remote_code true \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--load_from_disk \"\n",
    "base_cmd += f\"--dataset_path {hoc_path} \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--metric_path {f1_path} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# add exps\n",
    "seed_nb = 5\n",
    "cmds, exp_names = [], []\n",
    "for model_path in models_paths:\n",
    "    for seed in range(seed_nb):\n",
    "        cmd = base_cmd\n",
    "        cmd += f\"--model_name_or_path {model_path} \"\n",
    "        cmd += f\"--seed {seed} \"\n",
    "        # exp_name\n",
    "        exp_name = f\"{model_path.split('/')[-1]}_hoc_seed{seed}\"\n",
    "        exp_name += '_debug' if debug else ''\n",
    "        cmd += f\"--evaluate_cache_dir evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "        # out_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        cmd += f\"--output_dir {out_dir} \"\n",
    "        # fill lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 5 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e29ac59-a740-4b6e-9d50-5705e3037784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884279\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884280\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884281\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884282\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884283\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884284\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884285\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884286\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884287\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884288\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884290\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884291\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884292\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884293\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884294\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884296\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884297\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884298\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884299\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884300\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884301\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884303\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884305\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884307\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884308\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884310\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884311\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884312\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884313\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1884314\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec6a0d-b11a-432d-b4f6-eda1bf033616",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1e8c461-f68c-431a-b37d-de652c88ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "run_qa_path = f\"{root}/evaluation/run_qa.py\"\n",
    "f1_path = f\"{root}/evaluation/metrics/evaluate_accuracy.py\"\n",
    "datasets_paths = [f\"{root}/data/pubmed_qa\",f\"{root}/data/bioasq_task_b\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5da346ec-7b2a-4f4c-8b2c-84c0d6a1eab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : bert-base-uncased_pubmed_qa_seed0_debug\n",
      "1 : bert-base-uncased_pubmed_qa_seed1_debug\n",
      "2 : bert-base-uncased_pubmed_qa_seed2_debug\n",
      "3 : bert-base-uncased_pubmed_qa_seed3_debug\n",
      "4 : bert-base-uncased_pubmed_qa_seed4_debug\n",
      "5 : random_25%_pubmed_qa_seed0_debug\n",
      "6 : random_25%_pubmed_qa_seed1_debug\n",
      "7 : random_25%_pubmed_qa_seed2_debug\n",
      "8 : random_25%_pubmed_qa_seed3_debug\n",
      "9 : random_25%_pubmed_qa_seed4_debug\n",
      "10 : h-index_mid25%_pubmed_qa_seed0_debug\n",
      "11 : h-index_mid25%_pubmed_qa_seed1_debug\n",
      "12 : h-index_mid25%_pubmed_qa_seed2_debug\n",
      "13 : h-index_mid25%_pubmed_qa_seed3_debug\n",
      "14 : h-index_mid25%_pubmed_qa_seed4_debug\n",
      "15 : sjr_top25%_pubmed_qa_seed0_debug\n",
      "16 : sjr_top25%_pubmed_qa_seed1_debug\n",
      "17 : sjr_top25%_pubmed_qa_seed2_debug\n",
      "18 : sjr_top25%_pubmed_qa_seed3_debug\n",
      "19 : sjr_top25%_pubmed_qa_seed4_debug\n",
      "20 : sjr_mid25%_pubmed_qa_seed0_debug\n",
      "21 : sjr_mid25%_pubmed_qa_seed1_debug\n",
      "22 : sjr_mid25%_pubmed_qa_seed2_debug\n",
      "23 : sjr_mid25%_pubmed_qa_seed3_debug\n",
      "24 : sjr_mid25%_pubmed_qa_seed4_debug\n",
      "25 : h-index_mid50%_pubmed_qa_seed0_debug\n",
      "26 : h-index_mid50%_pubmed_qa_seed1_debug\n",
      "27 : h-index_mid50%_pubmed_qa_seed2_debug\n",
      "28 : h-index_mid50%_pubmed_qa_seed3_debug\n",
      "29 : h-index_mid50%_pubmed_qa_seed4_debug\n",
      "30 : h-index_top25%_pubmed_qa_seed0_debug\n",
      "31 : h-index_top25%_pubmed_qa_seed1_debug\n",
      "32 : h-index_top25%_pubmed_qa_seed2_debug\n",
      "33 : h-index_top25%_pubmed_qa_seed3_debug\n",
      "34 : h-index_top25%_pubmed_qa_seed4_debug\n",
      "35 : bert-base-uncased_bioasq_task_b_seed0_debug\n",
      "36 : bert-base-uncased_bioasq_task_b_seed1_debug\n",
      "37 : bert-base-uncased_bioasq_task_b_seed2_debug\n",
      "38 : bert-base-uncased_bioasq_task_b_seed3_debug\n",
      "39 : bert-base-uncased_bioasq_task_b_seed4_debug\n",
      "40 : random_25%_bioasq_task_b_seed0_debug\n",
      "41 : random_25%_bioasq_task_b_seed1_debug\n",
      "42 : random_25%_bioasq_task_b_seed2_debug\n",
      "43 : random_25%_bioasq_task_b_seed3_debug\n",
      "44 : random_25%_bioasq_task_b_seed4_debug\n",
      "45 : h-index_mid25%_bioasq_task_b_seed0_debug\n",
      "46 : h-index_mid25%_bioasq_task_b_seed1_debug\n",
      "47 : h-index_mid25%_bioasq_task_b_seed2_debug\n",
      "48 : h-index_mid25%_bioasq_task_b_seed3_debug\n",
      "49 : h-index_mid25%_bioasq_task_b_seed4_debug\n",
      "50 : sjr_top25%_bioasq_task_b_seed0_debug\n",
      "51 : sjr_top25%_bioasq_task_b_seed1_debug\n",
      "52 : sjr_top25%_bioasq_task_b_seed2_debug\n",
      "53 : sjr_top25%_bioasq_task_b_seed3_debug\n",
      "54 : sjr_top25%_bioasq_task_b_seed4_debug\n",
      "55 : sjr_mid25%_bioasq_task_b_seed0_debug\n",
      "56 : sjr_mid25%_bioasq_task_b_seed1_debug\n",
      "57 : sjr_mid25%_bioasq_task_b_seed2_debug\n",
      "58 : sjr_mid25%_bioasq_task_b_seed3_debug\n",
      "59 : sjr_mid25%_bioasq_task_b_seed4_debug\n",
      "60 : h-index_mid50%_bioasq_task_b_seed0_debug\n",
      "61 : h-index_mid50%_bioasq_task_b_seed1_debug\n",
      "62 : h-index_mid50%_bioasq_task_b_seed2_debug\n",
      "63 : h-index_mid50%_bioasq_task_b_seed3_debug\n",
      "64 : h-index_mid50%_bioasq_task_b_seed4_debug\n",
      "65 : h-index_top25%_bioasq_task_b_seed0_debug\n",
      "66 : h-index_top25%_bioasq_task_b_seed1_debug\n",
      "67 : h-index_top25%_bioasq_task_b_seed2_debug\n",
      "68 : h-index_top25%_bioasq_task_b_seed3_debug\n",
      "69 : h-index_top25%_bioasq_task_b_seed4_debug\n",
      "--------------------------\n",
      "Debugging with only exp n°5\n",
      "['random_25%_pubmed_qa_seed0_debug']\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_qa_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--trust_remote_code true \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--load_from_disk \"\n",
    "base_cmd += \"--text_column_names question,context \"\n",
    "base_cmd += \"--text_column_delimiter [SEP] \"\n",
    "base_cmd += \"--label_column_name answer \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--metric_path {f1_path} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# add exps\n",
    "seed_nb = 5\n",
    "cmds, exp_names = [], []\n",
    "for dataset_path in datasets_paths:\n",
    "    for model_path in models_paths:\n",
    "        for seed in range(seed_nb):\n",
    "            cmd = base_cmd\n",
    "            cmd += f\"--dataset_path {dataset_path} \"\n",
    "            cmd += f\"--model_name_or_path {model_path} \"\n",
    "            cmd += f\"--seed {seed} \"\n",
    "            # exp_name\n",
    "            exp_name = f\"{model_path.split('/')[-1]}_{dataset_path.split('/')[-1]}_seed{seed}\"\n",
    "            exp_name += '_debug' if debug else ''\n",
    "            cmd += f\"--evaluate_cache_dir evaluation/out/.evaluate_cache/{exp_name} \"\n",
    "            # out_dir\n",
    "            out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "            cmd += f\"--output_dir {out_dir} \"\n",
    "            # fill lists\n",
    "            if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "                print(exp_name, \"already finished\")\n",
    "                continue\n",
    "            cmds.append(cmd)\n",
    "            exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 5 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ee19836-c281-46c7-8179-752b8925b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1885469\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\",\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@v100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047a6bf-6810-4216-8ee5-1ec3fd85e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# job control and wandb synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a876d489-5b18-48e0-97da-f8b1b004cf41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1883403   gpu_p13 gpu-jupy  urz45id  R       0:10      1 r6i2n8\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e366883-5739-46c9-b808-5c25e2885d58",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/debug-cli.urz45id.log\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/e7w09v06 ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/iq5dhgys ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/2r6vti9k ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/w5yf82uy ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/0u0uvi9k ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/bi7vzkvm ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/2ittq5te ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ebe5rh7w ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/mozheaoc ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/s6ev0inu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/22wkcskd ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/xjgy4vjt ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/0cenke1a ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v8oloinm ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/xe21zcw0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ywrlg78t ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201011-eap68z7l/run-eap68z7l.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/vk9xl7xp ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/6vt1ewn0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v4ldqffk ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/1ofcoeb2 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7cdl59j5 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/hij3xha1 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201028-ivvfjtz3/run-ivvfjtz3.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7dqslo30 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/gg56hqyx ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/i01xgln1 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/kdtu7e5i ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/59owln2i ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/6lflx4e7 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201102-mb8ts4jf/run-mb8ts4jf.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/y74zhlzt ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/1kjmcaxu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201114-9sujb98z/run-9sujb98z.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/t3owgzxn ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/6srod422 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ii40k12l ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/keg9p8wp ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/qbdd4j9e ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/s4bwfaaq ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7v0gmvfg ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ns45dil0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/rpy7llg9 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/qil2nlbh ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201207-651zgzrx/run-651zgzrx.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/nacwekju ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/vlgh874u ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/zd0unls7 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/02z6lre9 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ww42kcqr ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/aal5e1xa ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/jtyd4pa6 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/aux2kvsc ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/b6lffqog ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/npb0wl0c ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/zzftvx9i ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/c7dbt3se ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/m4neyenf ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/u1m9by72 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/pf0wqbag ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v89mtm5e ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ibyythpp ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/rifpsgjk ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v19i1i7z ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/1yn77pqt ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7fm41xak ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201340-clbyppcx/run-clbyppcx.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/dyjdvk6r ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201355-5nniko2y/run-5nniko2y.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/68v4c6mg ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/hnyhc02r ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/odk7bphu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201420-7d4ucra9/run-7d4ucra9.wandb\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201420-bfcm6wk5/run-bfcm6wk5.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/eavwcbvd ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/m8dhc1ji ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/35wsdm73 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ej7oo7bd ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ksxoeju2 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/xss0xpvy ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/d455h8g7 ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/n97jsghp ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ra1pcfv6 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/4bheze3u ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/qsv5gjb0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/3mc3rr6t ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/t9ummrrv ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/fjcvwyxz ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/j7tbs8fr ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/spwtkrbv ... ^C\n",
      "\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "# sync weigths and biases\n",
    "# TODO : handle distributed logging (one wandb run for each GPU used currently)\n",
    "!wandb sync --include-offline wandb/offline-* --clean-force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f640be-90f6-46b7-a8a3-ee3b078f5f16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9cb63c1-4941-4a76-be5f-6f2bebf4b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel all my jobs\n",
    "!scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f387b2-27ac-4919-a0b3-66693ff420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete logs\n",
    "!rm -rf slurm/log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24378769-abf8-4694-bf87-797f1d0f5e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete slurm files\n",
    "!rm -rf slurm/*.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e795ecc8-a51d-402f-8366-876311b3b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete debug dirs\n",
    "!rm -rf pretraining/*_debug/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebfae16-93c2-404a-b61e-018e7e295b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf evaluation/out/*_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd51326a-116b-452b-973e-a73eb55a3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete wandb run dir\n",
    "!rm -rf wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36513978-d88d-4dc5-96ab-087635cc1776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove data cache and tmp files\n",
    "!rm -rf data/.blurb_cache/seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b414574-1b8e-4c1c-93d3-ffef13acfb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf core-python-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb05bcc1-10d8-40c0-a4bd-64b26507f1a5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy eval output dirs to WORK\n",
    "out_dir = f\"{os.getenv('WORK')}/results/pretrain-med-data-qual\"\n",
    "for pred_f in glob.glob(\"evaluation/out/*/predict_results.json\"):\n",
    "    out_f = f\"{out_dir}/{pred_f.split('/')[-2]}.json\"\n",
    "    if not os.path.exists(out_f):\n",
    "        res = json.load(open(pred_f))\n",
    "        json.dump(res,open(out_f,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d8b10f-ad8f-48dc-bcd0-9ddfe343b7cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean evaluation output dirs\n",
    "for sub in os.listdir(f\"{results_dir}/{rdir}\"):\n",
    "    subpath = f\"{results_dir}/{rdir}/{sub}\"\n",
    "    if \"result\" not in sub :\n",
    "        if os.path.isdir(subpath):\n",
    "            shutil.rmtree(subpath)\n",
    "        else :\n",
    "            os.remove(subpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9594e73-a49d-41f8-aa34-c52f5ffe268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/out/bert-base-uncased_blurb-bc5chem_seed2/eval_results.json\n"
     ]
    }
   ],
   "source": [
    "for f in glob.glob(\"evaluation/out/*/*\"):\n",
    "    print(f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "197715c6-a656-41ad-b341-0f986ca6c80a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy models to WORK\n",
    "models_work = os.path.join(os.getenv(\"WORK\"),\"models\",\"perso\")\n",
    "for subp in os.listdir(\"pretraining\"):\n",
    "    if '.' in subp:continue\n",
    "    renamed = \"bert-bio_\" + subp.replace(\"_\",\"-\").replace(\"%\",\"\").replace(\"h-index\",\"hind\")\n",
    "    out_dir = os.path.join(models_work,renamed)\n",
    "    if os.path.exists(f\"pretraining/{subp}/model.safetensors\"):\n",
    "        if not os.path.isdir(\"out_dir\"):os.mkdir(out_dir)\n",
    "        for f in os.listdir(f\"pretraining/{subp}\"):\n",
    "            if \"checkpoint\" not in f:\n",
    "                shutil.copy(f\"pretraining/{subp}/{f}\",out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.2.0_py3.11.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.2.0_py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
