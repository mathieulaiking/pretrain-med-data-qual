{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e7beae-3b06-4282-93e3-b9c7ac918ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from idr_pytools import gpu_jobs_submitter\n",
    "\n",
    "# project root path\n",
    "dsdir = os.getenv(\"DSDIR\")\n",
    "scratch = os.getenv(\"SCRATCH\")\n",
    "\n",
    "root = os.path.join(scratch,\"pretrain-med-data-qual\")\n",
    "idr_models_dir = os.path.join(dsdir,\"HuggingFace_Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaaa70-65b5-4f87-8983-4f39f5f2a0bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# bert mlm pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8f86e-be94-4339-8be5-d7dc5ffd58fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## computing number of optimal steps and grad accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171c5ae-b85e-4a3e-8db0-9d2a1d2a1300",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define the number of optimal steps as the number of steps required to perform an entire epoch on the full PubMed dataset (Baseline last update january 2024).\n",
    "Following RoBERTa, we aim for an effective batch_size of 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efafc7-412f-4796-a8b4-bb2fee4e2710",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Pretraining Phases Hardware arguments are taken from [NVIDIA Pytorch BERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#pre-training-nvidia-dgx-a100-8x-a100-80gb) and [NVIDIA Tensorflow BioBERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/biobert/README.md#pre-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e078cec7-86cb-4b17-8beb-6ee86920490b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation needed for effective batch size of 8192, with 2*a100 GPUs with per device batch size of 32 =  128\n",
      "Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : 4194304\n",
      "Optimal step number with effective batch size of 8192 : 3598.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient accumulation\n",
    "sequence_length = 512\n",
    "gpu_model = \"a100\"\n",
    "max_batch_size_per_gpu = 32 # on A100 with 512 seq length\n",
    "gpu_nb = 2\n",
    "target_batch_size = 8192\n",
    "gradient_accumulation = target_batch_size // (gpu_nb*max_batch_size_per_gpu) \n",
    "print(f\"Gradient accumulation needed for effective batch size of {target_batch_size}, with {gpu_nb}*{gpu_model} GPUs with per device batch size of {max_batch_size_per_gpu} = \",gradient_accumulation)\n",
    "# Optimal steps number\n",
    "total_token_nb =  15888466068 # calculated number of tokens in pubmed\n",
    "train_token_nb = 0.95*total_token_nb\n",
    "token_per_step = target_batch_size * sequence_length\n",
    "optimal_train_step_nb = train_token_nb // token_per_step\n",
    "print(f\"Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : {token_per_step}\")\n",
    "print(f\"Optimal step number with effective batch size of {target_batch_size} : {optimal_train_step_nb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bec35-8863-41ef-a311-343f4f1b1d7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## defining arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e357672-8722-46ea-bf07-2a404a8aab0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "pubmed_path = f\"{root}/data/pubmed_preproc\"\n",
    "bert_path = f\"{idr_models_dir}/bert-base-uncased\"\n",
    "run_mlm_path = f\"{root}/pretraining/run_mlm_offline.py \"\n",
    "accuracy_path = f\"{root}/pretraining/accuracy.py\"\n",
    "out_dir_template = f\"{root}/pretraining/{{exp_name}} \"\n",
    "\n",
    "# Pretraining Phases Hardware Arguments \n",
    "gpu = \"a100\"\n",
    "n_gpu = 2\n",
    "sequence_length = 512\n",
    "batch_size = 32 # per device\n",
    "precision = \"fp16\"\n",
    "max_steps = 3598 if not debug else 100\n",
    "acc_steps = 128 if not debug else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b4ae9a2-64dd-485c-82c3-6a7303cfc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : none_all_debug\n",
      "1 : random_50%_debug\n",
      "2 : random_25%_debug\n",
      "3 : h-index_top50%_debug\n",
      "4 : h-index_mid50%_debug\n",
      "5 : h-index_top25%_debug\n",
      "6 : h-index_mid25%_debug\n",
      "7 : sjr_top25%_debug\n",
      "8 : sjr_mid25%_debug\n",
      "9 : sjr_top50%_debug\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = \"torchrun --standalone \"\n",
    "base_cmd += f\"--nproc_per_node {n_gpu} \"\n",
    "base_cmd += \"--nnodes 1 \"\n",
    "base_cmd += f\"{run_mlm_path} \"\n",
    "\n",
    "# Model Arguments\n",
    "base_cmd += f\"--model_name_or_path {bert_path} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += f\"--dataset_name {pubmed_path} \"\n",
    "base_cmd += f\"--metric_path {accuracy_path} \"\n",
    "base_cmd += f\"--max_eval_samples {25600} \" if debug else \"\"\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {sequence_length} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train \" \n",
    "base_cmd += \"--do_eval \"  \n",
    "base_cmd += \"--seed 42 \" \n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## BERT hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 1e-4 \" \n",
    "base_cmd += \"--weight_decay 0.01 \" \n",
    "base_cmd += \"--adam_beta1 0.9 \" \n",
    "base_cmd += \"--adam_beta2 0.999 \" \n",
    "base_cmd += \"--adam_epsilon 1e-6 \" # RoBERTa\n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "base_cmd += f\"--gradient_accumulation_steps {acc_steps} \" if acc_steps else \"\"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--max_steps {max_steps} \"\n",
    "base_cmd += f\"--warmup_steps {max_steps//10} \" # warmup for 10% of steps\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy no \" # no evaluation during training only at the end for perplexity\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--logging_steps 0.01 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--log_on_each_node false \"\n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Data Filters experiences\n",
    "bounds = {\n",
    "    \"none\":[\n",
    "        (None,None,\"all\"),\n",
    "    ],\n",
    "    \"random\":[\n",
    "        (0.0,0.5,\"50%\"),\n",
    "        (0.0,0.25,\"25%\")\n",
    "    ],\n",
    "    \"h-index\":[\n",
    "        (103,1400,\"top50%\"),\n",
    "        (53,190,\"mid50%\"),\n",
    "        (190,1400,\"top25%\"),\n",
    "        (77,142,\"mid25%\"),\n",
    "    ],\n",
    "    \"sjr\":[\n",
    "        (1.312,100.0,\"top25%\"),\n",
    "        (0.462,0.984,\"mid25%\"),\n",
    "        (0.759,100.0,\"top50%\"),\n",
    "    ]\n",
    "}\n",
    "cmds = []\n",
    "exp_names = []\n",
    "for metric, exps in bounds.items():\n",
    "    for lower_bound, upper_bound, bound_name in exps:\n",
    "        cmd = base_cmd\n",
    "        # filtering metric\n",
    "        if metric != \"none\": \n",
    "            cmd += f\"--filter_metric {metric} \"\n",
    "            cmd += f\"--filter_lower_threshold {lower_bound} \"\n",
    "            cmd += f\"--filter_upper_threshold {upper_bound} \"\n",
    "        else:\n",
    "            cmd += \"--streaming \"\n",
    "        # experience name\n",
    "        exp_name = f\"{metric}_{bound_name}\" \n",
    "        if debug : exp_name += \"_debug\"\n",
    "        # output_dir\n",
    "        cmd += f\"--output_dir  {out_dir_template.format(exp_name=exp_name)} \"\n",
    "        # wandb args\n",
    "        cmd += f\"--wandb_group {exp_name}_{gpu}x{n_gpu} \"\n",
    "        cmd += f\"--wandb_name {exp_name} \"\n",
    "        # append to lists\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "        \n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc582b13-fa85-4d71-a10f-8b662dcde050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['torchrun --standalone --nproc_per_node 4 --nnodes 1 /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/run_mlm_offline.py  --model_name_or_path /gpfsdswork/dataset/HuggingFace_Models/bert-base-uncased --dataset_name /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/data/pubmed_preproc --metric_path /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/accuracy.py --max_eval_samples 25600 --preprocessing_num_workers 8 --max_seq_length 512 --do_train --do_eval --seed 42 --overwrite_output_dir true --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 1e-4 --weight_decay 0.01 --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-6 --fp16 true --eval_accumulation_steps 2 --gradient_accumulation_steps 2 --max_steps 100 --warmup_steps 10 --evaluation_strategy no --logging_strategy steps --save_strategy steps --logging_steps 0.01 --save_steps 0.1 --logging_first_step true --log_on_each_node false --save_total_limit 2 --disable_tqdm true --report_to wandb --filter_metric sjr --filter_lower_threshold 0.759 --filter_upper_threshold 100.0 --output_dir  /gpfsscratch/rech/aro/urz45id/pretrain-med-data-qual/pretraining/sjr_top50%_debug  --wandb_group sjr_top50%_debug_a100x4 --wandb_name sjr_top50%_debug ']\n",
      "['sjr_top50%_debug']\n"
     ]
    }
   ],
   "source": [
    "if debug : \n",
    "    debug_ind = 9\n",
    "    cmds = [cmds[debug_ind]]\n",
    "    exp_names = [exp_names[debug_ind]]\n",
    "    print(cmds)\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d07e-4f5e-47ce-91be-b917ce65a387",
   "metadata": {
    "tags": []
   },
   "source": [
    "## launching jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c382f9-5cb3-431c-b57b-a95ac5e5b2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 4 GPUs distributed on 1 nodes with 4 tasks / 4 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1687501\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = f\"\"\"module load python/3.11.5\n",
    "conda activate transformers_latest\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :\n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\" if \"v100\" in gpu else gpu,\n",
    "        time_max=\"20:00:00\" if not debug else \"2:00:00\",\n",
    "        account=f\"aro@{gpu}\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd455b5-6404-4ef1-bde8-581b69616f1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# blurb ner evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69123ad3-bd4b-4e9f-8a05-76e622224901",
   "metadata": {
    "tags": []
   },
   "source": [
    "## define arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808de476-3a97-4ad5-8c5c-8d93eea067ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "cache_dir = f\"{root}/evaluation/cache\"\n",
    "run_ner_path = f\"{root}/evaluation/run_ner_offline.py \"\n",
    "seqeval_path = f\"{root}/evaluation/evaluate_seqeval.py\"\n",
    "out_dir_template = f\"{root}/evaluation/out/{{exp_name}}\"\n",
    "\n",
    "models_paths = [\n",
    "    f\"{idr_models_dir}/bert-base-uncased\",\n",
    "]\n",
    "\n",
    "# Pretraining Phases Hardware Arguments \n",
    "gpu = \"v100\"\n",
    "n_gpu = 1\n",
    "max_seq_length = 512\n",
    "batch_size = 16 # per device\n",
    "precision = \"fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7cbce7e-4a37-4f76-818d-37456feb9493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_blurb-bc5chem_seed0 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed1 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed2 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed3 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed4 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed1 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed2 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed3 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed0 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed2 already finished\n",
      "bert-base-uncased_blurb-bc2gm_seed4 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed1 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed2 already finished\n",
      "bert-base-uncased_blurb-jnlpba_seed4 already finished\n",
      "bert-base-uncased_blurb-ncbi_disease_seed0 already finished\n",
      "bert-base-uncased_blurb-ncbi_disease_seed3 already finished\n",
      "0 : bert-base-uncased_blurb-bc5disease_seed0\n",
      "1 : bert-base-uncased_blurb-bc5disease_seed4\n",
      "2 : bert-base-uncased_blurb-bc2gm_seed1\n",
      "3 : bert-base-uncased_blurb-bc2gm_seed3\n",
      "4 : bert-base-uncased_blurb-jnlpba_seed0\n",
      "5 : bert-base-uncased_blurb-jnlpba_seed3\n",
      "6 : bert-base-uncased_blurb-ncbi_disease_seed1\n",
      "7 : bert-base-uncased_blurb-ncbi_disease_seed2\n",
      "8 : bert-base-uncased_blurb-ncbi_disease_seed4\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_ner_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--cache_dir {cache_dir} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--seqeval_path {seqeval_path} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 3 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Different experiments Runs \n",
    "datasets_configs=[\n",
    "    (\"bigbio/blurb\",\"bc5chem\"),\n",
    "    (\"bigbio/blurb\",\"bc5disease\"),\n",
    "    (\"bigbio/blurb\",\"bc2gm\"),\n",
    "    (\"bigbio/blurb\",\"jnlpba\"),\n",
    "    (\"bigbio/blurb\",\"ncbi_disease\"),\n",
    "    #(\"bigbio/ebm_pico\", None),\n",
    "]\n",
    "seed_nb = 5\n",
    "\n",
    "cmds = []\n",
    "exp_names = []\n",
    "\n",
    "for model_path in models_paths :\n",
    "    for dataset_name, dataset_config in datasets_configs :\n",
    "        for seed in range(seed_nb):\n",
    "            cmd = base_cmd\n",
    "            cmd += f\"--dataset_name {dataset_name} \"\n",
    "            cmd += f\"--dataset_config_name {dataset_config} \" if dataset_config else \"\"\n",
    "            cmd += f\"--seed {seed} \"\n",
    "            cmd += f\"--model_name_or_path {model_path} \"\n",
    "            # TODO : special treatment for EBM-NLP PICO\n",
    "            # Experience name for output directory\n",
    "            exp_name = f\"{model_path.split('/')[-1]}_{dataset_name.split('/')[-1]}\"\n",
    "            exp_name += f\"-{dataset_config}\" if dataset_config else \"\"\n",
    "            exp_name += f\"_seed{seed}\"\n",
    "            exp_name += \"_debug\" if debug else \"\"\n",
    "            out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "            cmd += f\"--output_dir {out_dir} \"\n",
    "            # Weights and Biases\n",
    "            cmd += f\"--run_name {exp_name}\"\n",
    "            # fill lists\n",
    "            if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "                print(exp_name, \"already finished\")\n",
    "                continue\n",
    "            cmds.append(cmd)\n",
    "            exp_names.append(exp_name)\n",
    "\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 24 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22051d-4f0d-4832-9a18-f6eb6aab90e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## launch jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2d0d3d-424b-4694-9710-b81949dd93e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698870\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698871\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698873\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698875\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698877\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698878\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698879\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698882\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698884\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1698887\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = f\"\"\"module load python/3.11.5\n",
    "conda activate transformers_latest\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\" if \"v100\" in gpu else gpu,\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@{gpu}\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047a6bf-6810-4216-8ee5-1ec3fd85e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# job control and wandb synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a876d489-5b18-48e0-97da-f8b1b004cf41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1687330    gpu_p5 sjr_top2  urz45id PD       0:00      1 (Priority)\n",
      "           1687329    gpu_p5 h-index_  urz45id PD       0:00      1 (Priority)\n",
      "           1687328    gpu_p5 h-index_  urz45id PD       0:00      1 (Priority)\n",
      "           1687327    gpu_p5 h-index_  urz45id PD       0:00      1 (Priority)\n",
      "           1687326    gpu_p5 h-index_  urz45id PD       0:00      1 (Priority)\n",
      "           1687325    gpu_p5 random_2  urz45id PD       0:00      1 (Priority)\n",
      "           1687324    gpu_p5 random_5  urz45id PD       0:00      1 (Priority)\n",
      "           1687323    gpu_p5 none_all  urz45id PD       0:00      1 (Priority)\n",
      "           1687332    gpu_p5 sjr_top5  urz45id PD       0:00      1 (Priority)\n",
      "           1687331    gpu_p5 sjr_mid2  urz45id PD       0:00      1 (Priority)\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e366883-5739-46c9-b808-5c25e2885d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/debug-cli.urz45id.log\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/hk0w7rc9 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/zsc07uq6 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/iblriyv7 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/qzzxy11y ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/0uebz89q ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/met9stea ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/b7znukz0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240423_164946-krgwfod9/run-krgwfod9.wandb\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/ncic226t ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/ueww7xsl ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/8onv07xy ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/9fw6hnmz ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/k6myos0s ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/rloo2eog ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/saf4exwb ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/utk56m8j ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/k6az5x9l ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/l2aj4zks ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/cafj14v5 ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/uzwt7l5b ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/b9h971ax ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/zn777zir ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/ia1z0rvc ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/oauwe1pa ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/f8o3cgde ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/h1tcuygp ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/0i42cyfd ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/qugaoiia ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/gbm5bcdg ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/rjb81u08 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# sync weigths and biases\n",
    "# TODO : handle distributed logging (one wandb run for each GPU used currently)\n",
    "!wandb sync --include-offline wandb/offline-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f640be-90f6-46b7-a8a3-ee3b078f5f16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# clean logs, cache , wandb runs, cancel jobs, training debug checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f387b2-27ac-4919-a0b3-66693ff420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete logs\n",
    "!rm -rf slurm/log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24378769-abf8-4694-bf87-797f1d0f5e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete slurm files\n",
    "!rm -rf slurm/*.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e795ecc8-a51d-402f-8366-876311b3b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete debug training checkpoints\n",
    "!rm -rf pretraining/*_debug/checkpoint*\n",
    "!rm -rf pretraining/*_debug/*.bin\n",
    "!rm -rf pretraining/*_debug/*.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd51326a-116b-452b-973e-a73eb55a3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete wandb run dir\n",
    "!rm -rf wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36513978-d88d-4dc5-96ab-087635cc1776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove data cache and tmp files\n",
    "!rm -rf data/.cache\n",
    "!rm -rf data/pubmed_preproc/*/cache-*.arrow\n",
    "!rm -rf data/pubmed_preproc/*/tmp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b414574-1b8e-4c1c-93d3-ffef13acfb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf core-python-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade3e0c2-b215-4a7b-8997-0ab5703d8749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cancel all my jobs\n",
    "!scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d8b10f-ad8f-48dc-bcd0-9ddfe343b7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove pretraining dirs /!\\ to handle with care\n",
    "#!rm -rf pretraining/*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.2.0_py3.11.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.2.0_py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
