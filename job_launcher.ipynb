{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e7beae-3b06-4282-93e3-b9c7ac918ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from idr_pytools import gpu_jobs_submitter\n",
    "\n",
    "# project root path\n",
    "dsdir = os.getenv(\"DSDIR\")\n",
    "scratch = os.getenv(\"SCRATCH\")\n",
    "root = os.path.join(scratch,\"pretrain-med-data-qual\")\n",
    "idr_models_dir = os.path.join(dsdir,\"HuggingFace_Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaaa70-65b5-4f87-8983-4f39f5f2a0bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# bert mlm pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8f86e-be94-4339-8be5-d7dc5ffd58fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## computing number of optimal steps and grad accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171c5ae-b85e-4a3e-8db0-9d2a1d2a1300",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define the number of optimal steps as the number of steps required to perform an entire epoch on the full PubMed dataset (Baseline last update january 2024).\n",
    "Following RoBERTa, we aim for an effective batch_size of 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efafc7-412f-4796-a8b4-bb2fee4e2710",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Pretraining Phases Hardware arguments are taken from [NVIDIA Pytorch BERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#pre-training-nvidia-dgx-a100-8x-a100-80gb) and [NVIDIA Tensorflow BioBERT Language Modeling](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/biobert/README.md#pre-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e078cec7-86cb-4b17-8beb-6ee86920490b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation needed for effective batch size of 8192, with 2*a100 GPUs with per device batch size of 32 =  128\n",
      "Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : 4194304\n",
      "Optimal step number with effective batch size of 8192 : 3598.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient accumulation\n",
    "sequence_length = 512\n",
    "gpu_model = \"a100\"\n",
    "max_batch_size_per_gpu = 32 # on A100 with 512 seq length\n",
    "gpu_nb = 2\n",
    "target_batch_size = 8192\n",
    "gradient_accumulation = target_batch_size // (gpu_nb*max_batch_size_per_gpu) \n",
    "print(f\"Gradient accumulation needed for effective batch size of {target_batch_size}, with {gpu_nb}*{gpu_model} GPUs with per device batch size of {max_batch_size_per_gpu} = \",gradient_accumulation)\n",
    "# Optimal steps number\n",
    "total_token_nb =  15888466068 # calculated number of tokens in pubmed\n",
    "train_token_nb = 0.95*total_token_nb\n",
    "token_per_step = target_batch_size * sequence_length\n",
    "optimal_train_step_nb = train_token_nb // token_per_step\n",
    "print(f\"Number of tokens per step (in an effective batch, with grad acc and gpu parrallel) : {token_per_step}\")\n",
    "print(f\"Optimal step number with effective batch size of {target_batch_size} : {optimal_train_step_nb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bec35-8863-41ef-a311-343f4f1b1d7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## defining arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e357672-8722-46ea-bf07-2a404a8aab0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "pubmed_path = f\"{root}/data/pubmed_preproc\"\n",
    "bert_path = f\"{idr_models_dir}/bert-base-uncased\"\n",
    "run_mlm_path = f\"{root}/pretraining/run_mlm_offline.py \"\n",
    "accuracy_path = f\"{root}/pretraining/accuracy.py\"\n",
    "out_dir_template = f\"{root}/pretraining/{{exp_name}}\"\n",
    "\n",
    "# Pretraining Phases Hardware Arguments \n",
    "gpu = \"a100\"\n",
    "n_gpu = 2\n",
    "sequence_length = 512\n",
    "batch_size = 32 # per device\n",
    "precision = \"fp16\"\n",
    "max_steps = 3598 if not debug else 100\n",
    "acc_steps = 128 if not debug else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b4ae9a2-64dd-485c-82c3-6a7303cfc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none_all run train and eval\n",
      "random_50% run train and eval\n",
      "random_25% already finished\n",
      "h-index_top50% run train and eval\n",
      "h-index_mid50% run eval only\n",
      "h-index_top25% run eval only\n",
      "h-index_mid25% already finished\n",
      "sjr_top25% already finished\n",
      "sjr_mid25% already finished\n",
      "sjr_top50% run train and eval\n",
      "0 : none_all\n",
      "1 : random_50%\n",
      "2 : h-index_top50%\n",
      "3 : h-index_mid50%\n",
      "4 : h-index_top25%\n",
      "5 : sjr_top50%\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = \"torchrun --standalone \"\n",
    "base_cmd += f\"--nproc_per_node {n_gpu} \"\n",
    "base_cmd += \"--nnodes 1 \"\n",
    "base_cmd += f\"{run_mlm_path} \"\n",
    "\n",
    "# Model Arguments\n",
    "base_cmd += f\"--model_name_or_path {bert_path} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += f\"--dataset_name {pubmed_path} \"\n",
    "base_cmd += f\"--metric_path {accuracy_path} \"\n",
    "base_cmd += f\"--max_eval_samples {25600} \" if debug else \"\"\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {sequence_length} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--seed 42 \" \n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## BERT hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 1e-4 \" \n",
    "base_cmd += \"--weight_decay 0.01 \" \n",
    "base_cmd += \"--adam_beta1 0.9 \" \n",
    "base_cmd += \"--adam_beta2 0.999 \" \n",
    "base_cmd += \"--adam_epsilon 1e-6 \" # RoBERTa\n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "base_cmd += f\"--gradient_accumulation_steps {acc_steps} \" if acc_steps else \"\"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--max_steps {max_steps} \"\n",
    "base_cmd += f\"--warmup_steps {max_steps//10} \" # warmup for 10% of steps\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy no \" # no evaluation during training only at the end for perplexity\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--logging_steps 0.01 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--log_on_each_node false \"\n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Data Filters experiences\n",
    "bounds = {\n",
    "    \"none\":[\n",
    "        (None,None,\"all\"),\n",
    "    ],\n",
    "    \"random\":[\n",
    "        (0.0,0.5,\"50%\"),\n",
    "        (0.0,0.25,\"25%\")\n",
    "    ],\n",
    "    \"h-index\":[\n",
    "        (103,1400,\"top50%\"),\n",
    "        (53,190,\"mid50%\"),\n",
    "        (190,1400,\"top25%\"),\n",
    "        (77,142,\"mid25%\"),\n",
    "    ],\n",
    "    \"sjr\":[\n",
    "        (1.312,100.0,\"top25%\"),\n",
    "        (0.462,0.984,\"mid25%\"),\n",
    "        (0.759,100.0,\"top50%\"),\n",
    "    ]\n",
    "}\n",
    "cmds = []\n",
    "exp_names = []\n",
    "for metric, exps in bounds.items():\n",
    "    for lower_bound, upper_bound, bound_name in exps:\n",
    "        cmd = base_cmd\n",
    "        # filtering metric\n",
    "        if metric != \"none\": \n",
    "            cmd += f\"--filter_metric {metric} \"\n",
    "            cmd += f\"--filter_lower_threshold {lower_bound} \"\n",
    "            cmd += f\"--filter_upper_threshold {upper_bound} \"\n",
    "        else:\n",
    "            cmd += \"--streaming \"\n",
    "        # experience name\n",
    "        exp_name = f\"{metric}_{bound_name}\" \n",
    "        if debug : exp_name += \"_debug\"\n",
    "        # output_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        cmd += f\"--output_dir  {out_dir} \"\n",
    "        # wandb args\n",
    "        cmd += f\"--wandb_group {exp_name}_{gpu}x{n_gpu} \"\n",
    "        cmd += f\"--wandb_name {exp_name} \"\n",
    "        # append to lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"eval_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        elif os.path.exists(os.path.join(out_dir,\"train_results.json\")):\n",
    "            print(exp_name,\"run eval only\")\n",
    "            cmd += \"--do_eval \"  \n",
    "        else : \n",
    "            print(exp_name,\"run train and eval\")\n",
    "            cmd += \"--do_train \" \n",
    "            cmd += \"--do_eval \" \n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "        \n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc582b13-fa85-4d71-a10f-8b662dcde050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if debug : \n",
    "    debug_ind = 4\n",
    "    cmds = [cmds[debug_ind]]\n",
    "    exp_names = [exp_names[debug_ind]]\n",
    "    print(cmds)\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d07e-4f5e-47ce-91be-b917ce65a387",
   "metadata": {
    "tags": []
   },
   "source": [
    "## launching jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11c382f9-5cb3-431c-b57b-a95ac5e5b2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1857227\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1857228\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1857229\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1857230\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1857231\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1857232\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = \"\"\"module load python/3.11.5\n",
    "conda activate transformers_latest\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :\n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\" if \"v100\" in gpu else gpu,\n",
    "        time_max=\"20:00:00\" if not debug else \"2:00:00\",\n",
    "        account=f\"aro@{gpu}\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd455b5-6404-4ef1-bde8-581b69616f1d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# fine-tune blurb eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb07337-e312-45e2-bdb6-a1eb7d200ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = f\"{root}/data/.blurb_cache\"\n",
    "out_dir_template = f\"{root}/evaluation/out/{{exp_name}}\"\n",
    "models_paths = [\n",
    "    f\"{idr_models_dir}/bert-base-uncased\",\n",
    "    f\"{root}/pretraining/random_25%\",\n",
    "    f\"{root}/pretraining/h-index_mid25%\",\n",
    "    f\"{root}/pretraining/sjr_top25%\",\n",
    "    f\"{root}/pretraining/sjr_mid25%\",\n",
    "]\n",
    "\n",
    "# Pretraining Phases Hardware Arguments \n",
    "gpu = \"v100\"\n",
    "n_gpu = 1\n",
    "max_seq_length = 512\n",
    "batch_size = 16 # per device\n",
    "precision = \"fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69123ad3-bd4b-4e9f-8a05-76e622224901",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808de476-3a97-4ad5-8c5c-8d93eea067ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = False \n",
    "\n",
    "# Local Paths\n",
    "run_ner_path = f\"{root}/evaluation/run_ner_offline.py \"\n",
    "seqeval_path = f\"{root}/evaluation/metrics/evaluate_seqeval.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7cbce7e-4a37-4f76-818d-37456feb9493",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_blurb-bc5chem_seed0 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed1 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed2 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed3 already finished\n",
      "bert-base-uncased_blurb-bc5chem_seed4 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed0 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed1 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed2 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed3 already finished\n",
      "bert-base-uncased_blurb-bc5disease_seed4 already finished\n",
      "random_25%_blurb-bc5chem_seed0 already finished\n",
      "random_25%_blurb-bc5chem_seed1 already finished\n",
      "random_25%_blurb-bc5chem_seed2 already finished\n",
      "random_25%_blurb-bc5chem_seed3 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed0 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed1 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed2 already finished\n",
      "h-index_mid25%_blurb-bc5chem_seed3 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed0 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed1 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed3 already finished\n",
      "sjr_top25%_blurb-bc5chem_seed4 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed0 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed1 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed2 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed3 already finished\n",
      "sjr_mid25%_blurb-bc5chem_seed4 already finished\n",
      "sjr_mid25%_blurb-bc5disease_seed1 already finished\n",
      "0 : random_25%_blurb-bc5chem_seed4\n",
      "1 : random_25%_blurb-bc5disease_seed0\n",
      "2 : random_25%_blurb-bc5disease_seed1\n",
      "3 : random_25%_blurb-bc5disease_seed2\n",
      "4 : random_25%_blurb-bc5disease_seed3\n",
      "5 : random_25%_blurb-bc5disease_seed4\n",
      "6 : h-index_mid25%_blurb-bc5chem_seed4\n",
      "7 : h-index_mid25%_blurb-bc5disease_seed0\n",
      "8 : h-index_mid25%_blurb-bc5disease_seed1\n",
      "9 : h-index_mid25%_blurb-bc5disease_seed2\n",
      "10 : h-index_mid25%_blurb-bc5disease_seed3\n",
      "11 : h-index_mid25%_blurb-bc5disease_seed4\n",
      "12 : sjr_top25%_blurb-bc5chem_seed2\n",
      "13 : sjr_top25%_blurb-bc5disease_seed0\n",
      "14 : sjr_top25%_blurb-bc5disease_seed1\n",
      "15 : sjr_top25%_blurb-bc5disease_seed2\n",
      "16 : sjr_top25%_blurb-bc5disease_seed3\n",
      "17 : sjr_top25%_blurb-bc5disease_seed4\n",
      "18 : sjr_mid25%_blurb-bc5disease_seed0\n",
      "19 : sjr_mid25%_blurb-bc5disease_seed2\n",
      "20 : sjr_mid25%_blurb-bc5disease_seed3\n",
      "21 : sjr_mid25%_blurb-bc5disease_seed4\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_ner_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--cache_dir {cache_dir} \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += \"--preprocessing_num_workers 8 \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--seqeval_path {seqeval_path} \"\n",
    "base_cmd += \"--return_entity_level_metrics \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# Different experiments Runs \n",
    "datasets_configs=[\n",
    "    (\"bigbio/blurb\",\"bc5chem\"),\n",
    "    (\"bigbio/blurb\",\"bc5disease\"),\n",
    "    (\"bigbio/blurb\",\"bc2gm\"),\n",
    "    (\"bigbio/blurb\",\"jnlpba\"),\n",
    "    (\"bigbio/blurb\",\"ncbi_disease\"),\n",
    "]\n",
    "seed_nb = 5\n",
    "\n",
    "cmds = []\n",
    "exp_names = []\n",
    "\n",
    "for model_path in models_paths :\n",
    "    for dataset_name, dataset_config in datasets_configs :\n",
    "        for seed in range(seed_nb):\n",
    "            cmd = base_cmd\n",
    "            cmd += f\"--dataset_name {dataset_name} \"\n",
    "            cmd += f\"--dataset_config_name {dataset_config} \" if dataset_config else \"\"\n",
    "            cmd += f\"--seed {seed} \"\n",
    "            cmd += f\"--model_name_or_path {model_path} \"\n",
    "            # Experience name for output directory\n",
    "            exp_name = f\"{model_path.split('/')[-1]}_{dataset_name.split('/')[-1]}\"\n",
    "            exp_name += f\"-{dataset_config}\" if dataset_config else \"\"\n",
    "            exp_name += f\"_seed{seed}\"\n",
    "            exp_name += \"_debug\" if debug else \"\"\n",
    "            out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "            cmd += f\"--output_dir {out_dir} \"\n",
    "            # Weights and Biases\n",
    "            cmd += f\"--run_name {exp_name}\"\n",
    "            # fill lists\n",
    "            if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "                print(exp_name, \"already finished\")\n",
    "                continue\n",
    "            cmds.append(cmd)\n",
    "            exp_names.append(exp_name)\n",
    "\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 24 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af2d0d3d-424b-4694-9710-b81949dd93e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861713\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861715\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861716\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861721\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861722\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861724\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861726\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861727\n",
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1861728\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = f\"\"\"module load python/3.11.5\n",
    "conda activate transformers_latest\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\" if \"v100\" in gpu else gpu,\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@{gpu}\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cc9b0-3a23-4d02-a369-9dcde2f7662d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## biosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2736f356-2270-4810-a98c-ecdf862267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "run_biosses_path = f\"{root}/evaluation/run_biosses.py\"\n",
    "pearsonr_path = f\"{root}/evaluation/evaluate_pearsonr.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c27598-9343-45f4-9a48-ecec9ae41379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : bert-base-uncased_biosses_seed0_debug\n",
      "1 : bert-base-uncased_biosses_seed1_debug\n",
      "2 : bert-base-uncased_biosses_seed2_debug\n",
      "3 : bert-base-uncased_biosses_seed3_debug\n",
      "4 : bert-base-uncased_biosses_seed4_debug\n",
      "5 : bert-base-uncased_biosses_seed5_debug\n",
      "6 : bert-base-uncased_biosses_seed6_debug\n",
      "7 : bert-base-uncased_biosses_seed7_debug\n",
      "8 : bert-base-uncased_biosses_seed8_debug\n",
      "9 : bert-base-uncased_biosses_seed9_debug\n",
      "--------------------------\n",
      "Debugging with only exp n°0\n",
      "['bert-base-uncased_biosses_seed0_debug']\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"python {run_biosses_path} \"\n",
    "cmd += f\"--pearsonr_path {pearsonr_path} \"\n",
    "# add exps\n",
    "seed_nb = 10\n",
    "cmds, exp_names = [], []\n",
    "for model_path in models_paths:\n",
    "    for seed in range(seed_nb):\n",
    "        cmd += f\"--model_path {model_path} \"\n",
    "        # exp_name\n",
    "        exp_name = f\"{model_path.split('/')[-1]}_biosses_seed{seed}\"\n",
    "        exp_name += '_debug' if debug else ''\n",
    "        # out_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        # fill lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 0 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6c1a7ba-c4fb-46f4-aa89-78d7b8ca0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 1856912\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"pytorch-gpu/py3/2.2.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\" if \"v100\" in gpu else gpu,\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@{gpu}\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d17c5-7483-43f4-87ac-9f7ae4fa680f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84ad405f-0458-4997-a7a8-def02cad2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "run_hoc_path = f\"{root}/evaluation/run_hoc.py\"\n",
    "f1_path = f\"{root}/evaluation/metrics/evaluate_f1.py\"\n",
    "hoc_path = f\"{root}/data/hallmarks_of_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2637c0a0-73dd-44ff-a267-2379cc67994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : bert-base-uncased_hoc_seed0_debug\n",
      "1 : bert-base-uncased_hoc_seed1_debug\n",
      "2 : bert-base-uncased_hoc_seed2_debug\n",
      "3 : bert-base-uncased_hoc_seed3_debug\n",
      "4 : bert-base-uncased_hoc_seed4_debug\n",
      "--------------------------\n",
      "Debugging with only exp n°0\n",
      "['bert-base-uncased_hoc_seed0_debug']\n"
     ]
    }
   ],
   "source": [
    "# Torchrun (distributed training) Arguments\n",
    "base_cmd = f\"python {run_hoc_path} \"\n",
    "\n",
    "# Model Argument\n",
    "base_cmd += f\"--trust_remote_code true \"\n",
    "\n",
    "# Dataset Arguments\n",
    "base_cmd += f\"--dataset_path {hoc_path} \"\n",
    "base_cmd += f\"--max_seq_length {max_seq_length} \"\n",
    "base_cmd += f\"--metric_path {f1_path} \"\n",
    "\n",
    "# Training Arguments\n",
    "## Basic arguments \n",
    "base_cmd += \"--do_train --do_eval --do_predict \"\n",
    "base_cmd += \"--overwrite_output_dir true \" if debug else \"\" \n",
    "## Hyperparameters \n",
    "base_cmd += f\"--per_device_train_batch_size {batch_size} \" \n",
    "base_cmd += f\"--per_device_eval_batch_size {batch_size} \" \n",
    "base_cmd += \"--learning_rate 3e-5 \" \n",
    "## Efficiency / Memory\n",
    "base_cmd += f\"--{precision} true \"\n",
    "base_cmd += \"--eval_accumulation_steps 2 \"\n",
    "## Number of steps / epochs\n",
    "base_cmd += f\"--num_train_epochs 5 \"\n",
    "base_cmd += f\"--warmup_ratio 0.1 \"\n",
    "## Evaluation / Logging / Model Save\n",
    "base_cmd += \"--evaluation_strategy steps \"\n",
    "base_cmd += \"--logging_strategy steps \" \n",
    "base_cmd += \"--save_strategy steps \" \n",
    "base_cmd += \"--eval_steps 0.1 \"\n",
    "base_cmd += \"--logging_steps 0.1 \"\n",
    "base_cmd += \"--save_steps 0.1 \" \n",
    "base_cmd += \"--logging_first_step true \" \n",
    "base_cmd += \"--save_total_limit 2 \"\n",
    "base_cmd += \"--load_best_model_at_end true \"\n",
    "## Experiment Visualisation\n",
    "base_cmd += \"--disable_tqdm true \"\n",
    "base_cmd += \"--report_to wandb \"\n",
    "\n",
    "# add exps\n",
    "seed_nb = 5\n",
    "cmds, exp_names = [], []\n",
    "for model_path in models_paths:\n",
    "    for seed in range(seed_nb):\n",
    "        cmd += f\"--model_path {model_path} \"\n",
    "        # exp_name\n",
    "        exp_name = f\"{model_path.split('/')[-1]}_hoc_seed{seed}\"\n",
    "        exp_name += '_debug' if debug else ''\n",
    "        # out_dir\n",
    "        out_dir = out_dir_template.format(exp_name=exp_name)\n",
    "        # fill lists\n",
    "        if os.path.exists(os.path.join(out_dir,\"predict_results.json\")):\n",
    "            print(exp_name, \"already finished\")\n",
    "            continue\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "# Display experiences and chosen debug\n",
    "for i,e in enumerate(exp_names):print(i,\":\",e)\n",
    "if debug : \n",
    "    i = 0 # CHANGE THIS VALUE TO CHOOSE WHICH EXPERIENCE TO DEBUG\n",
    "    cmds = [cmds[i]]\n",
    "    exp_names = [exp_names[i]]\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Debugging with only exp n°{i}\")\n",
    "    print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e29ac59-a740-4b6e-9d50-5705e3037784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 1 GPUs distributed on 1 nodes with 1 tasks / 1 gpus per node and 10 cpus per task\n",
      "Submitted batch job 1858036\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = f\"\"\"module load python/3.11.5\n",
    "conda activate transformers_latest\"\"\"\n",
    "\n",
    "for cmd,exp_name in zip(cmds, exp_names) :   \n",
    "    # change log filename according to experience\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    # send job\n",
    "    job_ids = gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name = exp_name,\n",
    "        module = \"cuda/12.1.0\",\n",
    "        n_gpu = n_gpu,\n",
    "        qos = \"qos_gpu-dev\" if debug else \"qos_gpu-t3\",\n",
    "        constraint = \"v100-32g\" if \"v100\" in gpu else gpu,\n",
    "        time_max=\"02:00:00\",\n",
    "        account=f\"aro@{gpu}\",\n",
    "        email=\"mathieu.lai-king@lisn.upsaclay.fr\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec6a0d-b11a-432d-b4f6-eda1bf033616",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8c461-f68c-431a-b37d-de652c88ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to modify according to needs\n",
    "debug = True \n",
    "\n",
    "# Local Paths\n",
    "run_hoc_path = f\"{root}/evaluation/run_qa.py\"\n",
    "f1_path = f\"{root}/evaluation/metrics/evaluate_f1.py\"\n",
    "datasets_paths = [\n",
    "    f\"{root}/data/hallmarks_of_cancer\",\n",
    "    f\"{root}/data/hallmarks_of_cancer\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da346ec-7b2a-4f4c-8b2c-84c0d6a1eab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d377f5-25eb-4ec2-a7d2-972ea653c7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9047a6bf-6810-4216-8ee5-1ec3fd85e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# job control and wandb synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a876d489-5b18-48e0-97da-f8b1b004cf41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1857232    gpu_p5 sjr_top5  urz45id PD       0:00      1 (Priority)\n",
      "           1857231    gpu_p5 h-index_  urz45id PD       0:00      1 (Priority)\n",
      "           1857228    gpu_p5 random_5  urz45id PD       0:00      1 (Priority)\n",
      "           1857229    gpu_p5 h-index_  urz45id PD       0:00      1 (Priority)\n",
      "           1857230    gpu_p5 h-index_  urz45id PD       0:00      1 (Priority)\n",
      "           1857227    gpu_p5 none_all  urz45id PD       0:00      1 (Priority)\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e366883-5739-46c9-b808-5c25e2885d58",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/debug-cli.urz45id.log\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/e7w09v06 ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/iq5dhgys ... done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/2r6vti9k ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/pretrain-med-data-qual/runs/w5yf82uy ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/0u0uvi9k ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/bi7vzkvm ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/2ittq5te ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ebe5rh7w ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/mozheaoc ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/s6ev0inu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/22wkcskd ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/xjgy4vjt ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/0cenke1a ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v8oloinm ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/xe21zcw0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ywrlg78t ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201011-eap68z7l/run-eap68z7l.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/vk9xl7xp ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/6vt1ewn0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v4ldqffk ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/1ofcoeb2 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7cdl59j5 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/hij3xha1 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201028-ivvfjtz3/run-ivvfjtz3.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7dqslo30 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/gg56hqyx ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/i01xgln1 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/kdtu7e5i ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/59owln2i ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/6lflx4e7 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201102-mb8ts4jf/run-mb8ts4jf.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/y74zhlzt ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/1kjmcaxu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201114-9sujb98z/run-9sujb98z.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/t3owgzxn ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/6srod422 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ii40k12l ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/keg9p8wp ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/qbdd4j9e ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/s4bwfaaq ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7v0gmvfg ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ns45dil0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/rpy7llg9 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/qil2nlbh ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201207-651zgzrx/run-651zgzrx.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/nacwekju ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/vlgh874u ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/zd0unls7 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/02z6lre9 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ww42kcqr ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/aal5e1xa ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/jtyd4pa6 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/aux2kvsc ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/b6lffqog ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/npb0wl0c ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/zzftvx9i ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/c7dbt3se ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/m4neyenf ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/u1m9by72 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/pf0wqbag ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v89mtm5e ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ibyythpp ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/rifpsgjk ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/v19i1i7z ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/1yn77pqt ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/7fm41xak ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201340-clbyppcx/run-clbyppcx.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/dyjdvk6r ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201355-5nniko2y/run-5nniko2y.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/68v4c6mg ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/hnyhc02r ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/odk7bphu ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201420-7d4ucra9/run-7d4ucra9.wandb\n",
      ".wandb file is empty (header is 0 bytes instead of the expected 7), skipping: /gpfsssd/scratch/rech/aro/urz45id/pretrain-med-data-qual/wandb/offline-run-20240512_201420-bfcm6wk5/run-bfcm6wk5.wandb\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/eavwcbvd ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/m8dhc1ji ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/35wsdm73 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ej7oo7bd ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ksxoeju2 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/xss0xpvy ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/d455h8g7 ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/n97jsghp ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/ra1pcfv6 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/4bheze3u ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/qsv5gjb0 ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/3mc3rr6t ... done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/t9ummrrv ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/fjcvwyxz ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/j7tbs8fr ... \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "done.\n",
      "Syncing: https://wandb.ai/laiking/blurb-ner-finetune/runs/spwtkrbv ... ^C\n",
      "\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "# sync weigths and biases\n",
    "# TODO : handle distributed logging (one wandb run for each GPU used currently)\n",
    "!wandb sync --include-offline wandb/offline-* --mark-synced --clean-force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f640be-90f6-46b7-a8a3-ee3b078f5f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb63c1-4941-4a76-be5f-6f2bebf4b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel all my jobs\n",
    "!scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6f387b2-27ac-4919-a0b3-66693ff420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete logs\n",
    "!rm -rf slurm/log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24378769-abf8-4694-bf87-797f1d0f5e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete slurm files\n",
    "!rm -rf slurm/*.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e795ecc8-a51d-402f-8366-876311b3b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete debug pretraining dirs\n",
    "!rm -rf pretraining/*_debug/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd51326a-116b-452b-973e-a73eb55a3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete wandb run dir\n",
    "!rm -rf wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36513978-d88d-4dc5-96ab-087635cc1776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove data cache and tmp files\n",
    "!rm -rf data/.blurb_cache/seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b414574-1b8e-4c1c-93d3-ffef13acfb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf core-python-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d8b10f-ad8f-48dc-bcd0-9ddfe343b7cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean evaluation output dirs and copy to WORK\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "\n",
    "permanent_res_dir = os.path.join(os.getenv(\"WORK\"),\"results\",\"pretrain-med-data-qual\")\n",
    "results_dir = \"evaluation/out\"\n",
    "for rdir in os.listdir(results_dir):\n",
    "    if os.path.exists(f\"{results_dir}/{rdir}/predict_results.json\"):\n",
    "        output = os.path.join(permanent_res_dir,rdir+\".json\")\n",
    "        test_res = json.load(open(f\"{results_dir}/{rdir}/predict_results.json\"))\n",
    "        if not os.path.exists(output):\n",
    "            json.dump(test_res, open(output,\"w\"))\n",
    "        for sub in os.listdir(f\"{results_dir}/{rdir}\"):\n",
    "            subpath = f\"{results_dir}/{rdir}/{sub}\"\n",
    "            if \"result\" not in sub :\n",
    "                if os.path.isdir(subpath):\n",
    "                    shutil.rmtree(subpath)\n",
    "                else :\n",
    "                    os.remove(subpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "197715c6-a656-41ad-b341-0f986ca6c80a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy models to WORK\n",
    "models_work = os.path.join(os.getenv(\"WORK\"),\"models\",\"perso\")\n",
    "for subp in os.listdir(\"pretraining\"):\n",
    "    if '.' in subp:continue\n",
    "    renamed = \"bert-bio_\" + subp.replace(\"_\",\"-\").replace(\"%\",\"\").replace(\"h-index\",\"hind\")\n",
    "    out_dir = os.path.join(models_work,renamed)\n",
    "    if os.path.exists(f\"pretraining/{subp}/model.safetensors\"):\n",
    "        if not os.path.isdir(\"out_dir\"):os.mkdir(out_dir)\n",
    "        for f in os.listdir(f\"pretraining/{subp}\"):\n",
    "            if \"checkpoint\" not in f:\n",
    "                \n",
    "                shutil.copy(f\"pretraining/{subp}/{f}\",out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.2.0_py3.11.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.2.0_py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
